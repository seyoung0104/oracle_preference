{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91ba088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FOMC 'Speeches' (연설문) 스크래핑 시작 (v4) ---\n",
      "대상 기간: 2000년 ~ 2025년\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  46%|████▌     | 12/26 [15:21<13:56, 59.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [오류] 링크에서 텍스트 추출 실패: https://www.federalreserve.gov/newsevents/speech/bernanke20131119a.htm (HTTPSConnectionPool(host='www.federalreserve.gov', port=443): Read timed out. (read timeout=10))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  81%|████████  | 21/26 [25:27<05:00, 60.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2005년 - 'eventlist' 또는 'article' div를 찾지 못함.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  85%|████████▍ | 22/26 [25:30<02:51, 42.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2004년 - 'eventlist' 또는 'article' div를 찾지 못함.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  88%|████████▊ | 23/26 [25:33<01:32, 30.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2003년 - 'eventlist' 또는 'article' div를 찾지 못함.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  92%|█████████▏| 24/26 [25:34<00:43, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2002년 - 'eventlist' 또는 'article' div를 찾지 못함.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  96%|█████████▌| 25/26 [25:36<00:15, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2001년 - 'eventlist' 또는 'article' div를 찾지 못함.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집: 100%|██████████| 26/26 [25:38<00:00, 59.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 2000년 - 'eventlist' 또는 'article' div를 찾지 못함.\n",
      "\n",
      "--- (2000년 ~ 2025년) 총 1268개 항목 중 1267개의 'Speeches' 문서를 수집했습니다.\n",
      "\n",
      "--- 모든 'Speeches' 작업 완료 ---\n",
      "최종 데이터가 'C:\\Users\\82109\\Downloads\\statements\\fomc_speeches_2000-present.jsonl' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FOMC 'Speeches' (연설문) 전용 크롤러 (2000 ~ 현재)\n",
    "#\n",
    "# [2025-10-30 v4 수정]\n",
    "# - 0개 수집 문제 해결 (debug HTML 파일 기반)\n",
    "# - v3의 'news-item' 파싱 로직을 버리고,\n",
    "#   'eventlist', 'eventlist__time', 'eventlist__event', 'news__speaker'\n",
    "#   클래스를 사용하도록 파싱 로직 전면 수정.\n",
    "# - 날짜 형식을 '%m/%d/%y' (2자리 연도) -> '%m/%d/%Y' (4자리 연도)로 수정.\n",
    "#\n",
    "# [동작 방식 (하이브리드)]\n",
    "# 2000년부터 2025년까지 루프를 돌면서,\n",
    "# 1. '.../{year}-speeches.htm' (2011-현재) 패턴을 먼저 시도\n",
    "# 2. 404가 발생하면 '.../{year}speech.htm' (2000-2010) 패턴을 시도\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install requests beautifulsoup4 PyMuPDF tqdm`을 실행하여\n",
    "#    라이브러리를 설치하세요.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "URL_TEMPLATE_NEW = \"https://www.federalreserve.gov/newsevents/speech/{year}-speeches.htm\"\n",
    "URL_TEMPLATE_OLD = \"https://www.federalreserve.gov/newsevents/speech/{year}speech.htm\"\n",
    "\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"fomc_speeches_2000-present.jsonl\")\n",
    "START_YEAR = 2000 \n",
    "END_YEAR = datetime.datetime.now().year\n",
    "\n",
    "# --- 2. 헬퍼 함수 (Helper Functions) ---\n",
    "\n",
    "def save_to_jsonl(data, outfile):\n",
    "    \"\"\"데이터를 JSON Lines (.jsonl) 파일에 추가합니다.\"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(outfile)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        with open(outfile, 'a', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"  [오류] 파일 쓰기 실패: {e}\")\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"지정된 URL의 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code == 404:\n",
    "            return None # 404는 흔한 경우이므로 조용히 None 반환\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding \n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] URL에 접근할 수 없습니다: {url} ({e})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"불필요한 공백과 줄바꿈 문자를 정제합니다.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_link(link_url):\n",
    "    \"\"\"HTML 또는 PDF 링크에서 텍스트를 깨끗하게 추출합니다.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        full_url = urljoin(BASE_URL, link_url)\n",
    "        \n",
    "        response = requests.get(full_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \".pdf\" in full_url.lower():\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content = soup.find('div', id='article')\n",
    "            if not content:\n",
    "                content_paragraphs = soup.find_all('p')\n",
    "                if content_paragraphs:\n",
    "                    text = \"\\n\".join([p.get_text() for p in content_paragraphs])\n",
    "                else:\n",
    "                    content = soup.find('body')\n",
    "            \n",
    "            if content and not text:\n",
    "                for script_or_style in content(['script', 'style']):\n",
    "                    script_or_style.decompose()\n",
    "                text = content.get_text(separator=\"\\n\", strip=True)\n",
    "            elif not text:\n",
    "                text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] 링크에서 텍스트 추출 실패: {full_url} ({e})\")\n",
    "        \n",
    "    time.sleep(0.5) \n",
    "    return clean_text(text)\n",
    "\n",
    "# --- 3. 메인 스크래핑 함수 (v4: 'eventlist' 구조 파싱) ---\n",
    "def scrape_all_speeches(output_file):\n",
    "    print(f\"--- FOMC 'Speeches' (연설문) 스크래핑 시작 (v4) ---\")\n",
    "    print(f\"대상 기간: {START_YEAR}년 ~ {END_YEAR}년\")\n",
    "    \n",
    "    scraped_count = 0\n",
    "    total_items_processed = 0\n",
    "    \n",
    "    for year in tqdm(range(END_YEAR, START_YEAR - 1, -1), desc=\"연도별 수집\"):\n",
    "        \n",
    "        url = URL_TEMPLATE_NEW.format(year=year)\n",
    "        soup = get_soup(url)\n",
    "        \n",
    "        if soup is None:\n",
    "            time.sleep(0.5)\n",
    "            url = URL_TEMPLATE_OLD.format(year=year)\n",
    "            soup = get_soup(url)\n",
    "\n",
    "        if soup is None:\n",
    "            print(f\"  [정보] {year}년 - 두 URL 패턴 모두 찾지 못함. 건너뜁니다.\")\n",
    "            continue\n",
    "            \n",
    "        # [v4 수정] 'eventlist' div를 찾음\n",
    "        eventlist_container = soup.find('div', class_='eventlist')\n",
    "        if not eventlist_container:\n",
    "            eventlist_container = soup.find('div', id='article') # Fallback\n",
    "            if not eventlist_container:\n",
    "                print(f\"  [경고] {year}년 - 'eventlist' 또는 'article' div를 찾지 못함.\")\n",
    "                continue\n",
    "\n",
    "        # [v4 수정] eventlist 안의 모든 'row'가 개별 연설 항목임\n",
    "        speech_items = eventlist_container.find_all('div', class_='row')\n",
    "            \n",
    "        for item in speech_items:\n",
    "            # [v4 수정] 'time' 태그와 'eventlist__event' 클래스로 검색\n",
    "            date_element = item.find('time')\n",
    "            event_element = item.find('div', class_='eventlist__event')\n",
    "            \n",
    "            if not date_element or not event_element:\n",
    "                continue # 날짜나 이벤트 내용이 없는 row (예: 헤더, 공백)는 건너뜀\n",
    "\n",
    "            try:\n",
    "                date_text = date_element.get_text(strip=True)\n",
    "                # [v4 수정] '%m/%d/%Y' (4자리 연도)로 파싱 (예: 12/3/2024)\n",
    "                meeting_date = datetime.datetime.strptime(date_text, '%m/%d/%Y')\n",
    "                \n",
    "                # 수집 중인 연도와 날짜가 일치하는지 확인\n",
    "                if meeting_date.year != year:\n",
    "                    continue \n",
    "\n",
    "                # [v4 수정] 'eventlist__event' 안의 첫 <p><a> 태그\n",
    "                title_link = event_element.find('p').find('a')\n",
    "                if not title_link:\n",
    "                    continue\n",
    "                    \n",
    "                # [v4 수정] 'news__speaker' 클래스로 연설자 검색\n",
    "                speaker_element = event_element.find('p', class_='news__speaker')\n",
    "\n",
    "                link_url = title_link.get('href')\n",
    "                title = clean_text(title_link.get_text())\n",
    "                speaker = clean_text(speaker_element.get_text()) if speaker_element else \"Unknown\"\n",
    "\n",
    "                total_items_processed += 1\n",
    "                text = extract_text_from_link(link_url)\n",
    "                \n",
    "                if text:\n",
    "                    data = {\n",
    "                        \"date\": meeting_date.strftime('%Y-%m-%d'), \n",
    "                        \"year\": meeting_date.year,\n",
    "                        \"source_type\": \"FOMC Speech\",\n",
    "                        \"speaker\": speaker,\n",
    "                        \"title\": title,\n",
    "                        \"url\": urljoin(BASE_URL, link_url),\n",
    "                        \"text\": text\n",
    "                    }\n",
    "                    save_to_jsonl(data, output_file)\n",
    "                    scraped_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                # print(f\"  [정보] {year}년 - 연설문 항목 파싱 실패: {e}\")\n",
    "                pass\n",
    "                        \n",
    "    print(f\"\\n--- (2000년 ~ {END_YEAR}년) 총 {total_items_processed}개 항목 중 {scraped_count}개의 'Speeches' 문서를 수집했습니다.\")\n",
    "\n",
    "# --- 4. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "        print(f\"'{DOWNLOADS_DIR}' 폴더를 생성했습니다.\")\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"기존 파일 '{OUTPUT_FILE}'을 삭제했습니다. 새로 수집합니다.\")\n",
    "    \n",
    "    scrape_all_speeches(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n--- 모든 'Speeches' 작업 완료 ---\")\n",
    "    print(f\"최종 데이터가 '{OUTPUT_FILE}' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e0497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
