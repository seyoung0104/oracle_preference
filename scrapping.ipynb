{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc1bb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 설정 (Configuration) ---\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "# 2000년부터 2019년까지의 URL 패턴\n",
    "HISTORICAL_URL_PATTERN = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "# 2020년부터 현재까지의 URL\n",
    "RECENT_URL = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "\n",
    "# 사용자의 'Downloads' 폴더 경로를 동적으로 찾습니다.\n",
    "# os.path.expanduser('~')는 홈 디렉터리 (예: C:\\Users\\YourName or /Users/YourName)를 반환합니다.\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements') # 'statements' 폴더 경로 추가\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"fomc_statements_2000-present.jsonl\")\n",
    "START_YEAR = 2000\n",
    "END_YEAR = datetime.datetime.now().year\n",
    "\n",
    "# --- 헬퍼 함수 (Helper Functions) ---\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"지정된 URL의 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()  # HTTP 오류가 있으면 예외 발생\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] URL에 접근할 수 없습니다: {url} ({e})\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_link(link_url):\n",
    "    \"\"\"HTML 또는 PDF 링크에서 텍스트를 추출합니다.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # 링크가 상대 경로일 수 있으므로 절대 경로로 변환\n",
    "        full_url = urljoin(BASE_URL, link_url)\n",
    "        \n",
    "        response = requests.get(full_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \".pdf\" in full_url.lower():\n",
    "            # PDF 처리\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        else:\n",
    "            # HTML 처리\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Fed 보도자료의 메인 콘텐츠 영역을 타겟팅\n",
    "            content_div = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8')\n",
    "            if content_div:\n",
    "                text = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                # 대체 타겟팅 (구조가 다를 경우)\n",
    "                text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "                print(f\"  [경고] HTML에서 기본 콘텐츠 영역을 찾지 못했습니다. 전체 텍스트를 추출합니다. URL: {full_url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] 링크에서 텍스트 추출 실패: {full_url} ({e})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] 파일 처리 중 알 수 없는 오류: {full_url} ({e})\")\n",
    "        \n",
    "    time.sleep(0.5)  # 서버에 부담을 주지 않기 위해 잠시 대기\n",
    "    return text\n",
    "\n",
    "def save_to_jsonl(data):\n",
    "    \"\"\"데이터를 JSON Lines (.jsonl) 파일에 추가합니다.\"\"\"\n",
    "    try:\n",
    "        # 파일 저장 전 폴더가 있는지 확인하고 없으면 생성\n",
    "        output_dir = os.path.dirname(OUTPUT_FILE)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"  [정보] '{output_dir}' 폴더를 생성했습니다.\")\n",
    "            \n",
    "        with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"[오류] 파일 쓰기 실패: {e}\")\n",
    "\n",
    "# --- 메인 크롤링 함수 ---\n",
    "\n",
    "def crawl_historical_years(start_year, end_year):\n",
    "    \"\"\"2000년부터 2019년까지의 과거 데이터를 크롤링합니다.\"\"\"\n",
    "    print(f\"--- {start_year}-{end_year} 과거 데이터 크롤링 시작 ---\")\n",
    "    \n",
    "    for year in tqdm(range(start_year, end_year + 1), desc=\"연도별 진행\"):\n",
    "        url = HISTORICAL_URL_PATTERN.format(year=year)\n",
    "        soup = get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            continue\n",
    "            \n",
    "        # Fed 웹사이트의 과거 데이터 구조 (패널 형태)\n",
    "        panels = soup.find_all('div', class_='panel')\n",
    "        \n",
    "        for panel in panels:\n",
    "            # 날짜 정보 추출 (다양한 형식이 존재할 수 있음)\n",
    "            date_element = panel.find(['div', 'h5'], class_=['panel-heading', 'fomc-meeting__date'])\n",
    "            date_text = date_element.get_text(strip=True) if date_element else str(year) # 날짜를 못찾으면 연도라도 기록\n",
    "            \n",
    "            # 성명서(Statement) 링크 찾기\n",
    "            statement_link = panel.find('a', string=lambda t: t and 'statement' in t.lower())\n",
    "            \n",
    "            if statement_link:\n",
    "                link_url = statement_link.get('href')\n",
    "                if link_url:\n",
    "                    print(f\"  > {year} ({date_text}) 성명서 발견. 텍스트 추출 중...\")\n",
    "                    text = extract_text_from_link(link_url)\n",
    "                    \n",
    "                    if text:\n",
    "                        data = {\n",
    "                            \"date\": date_text,\n",
    "                            \"year\": year,\n",
    "                            \"source_type\": \"FOMC Statement\",\n",
    "                            \"url\": urljoin(BASE_URL, link_url),\n",
    "                            \"text\": text\n",
    "                        }\n",
    "                        save_to_jsonl(data)\n",
    "\n",
    "def crawl_recent_years(start_year, end_year):\n",
    "    \"\"\"2020년부터 현재까지의 최근 데이터를 크롤링합니다.\"\"\"\n",
    "    print(f\"--- {start_year}-{end_year} 최근 데이터 크롤링 시작 ---\")\n",
    "    \n",
    "    soup = get_soup(RECENT_URL)\n",
    "    if not soup:\n",
    "        print(\"[오류] 최근 데이터 페이지에 접근할 수 없습니다. 크롤링 종료.\")\n",
    "        return\n",
    "\n",
    "    # 최근 데이터는 'fomc-meeting' 클래스 섹션에 연도별로 그룹화되어 있음\n",
    "    year_sections = soup.find_all('div', class_='fomc-meeting--year-section')\n",
    "    \n",
    "    for year_section in year_sections:\n",
    "        try:\n",
    "            year = int(year_section.get('id'))\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "            \n",
    "        if year < start_year or year > end_year:\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- {year}년 데이터 처리 중 ---\")\n",
    "        \n",
    "        meetings = year_section.find_all('div', class_='fomc-meeting')\n",
    "        \n",
    "        for meeting in tqdm(meetings, desc=f\"{year}년 회의\"):\n",
    "            date_element = meeting.find('div', class_='fomc-meeting__date')\n",
    "            date_text = date_element.get_text(strip=True) if date_element else str(year)\n",
    "            \n",
    "            # 성명서(Statement) 링크 찾기\n",
    "            statement_link = meeting.find('a', string=lambda t: t and 'statement' in t.lower())\n",
    "            \n",
    "            if statement_link:\n",
    "                link_url = statement_link.get('href')\n",
    "                if link_url:\n",
    "                    print(f\"  > {year} ({date_text}) 성명서 발견. 텍스트 추출 중...\")\n",
    "                    text = extract_text_from_link(link_url)\n",
    "                    \n",
    "                    if text:\n",
    "                        data = {\n",
    "                            \"date\": date_text,\n",
    "                            \"year\": year,\n",
    "                            \"source_type\": \"FOMC Statement\",\n",
    "                            \"url\": urljoin(BASE_URL, link_url),\n",
    "                            \"text\": text\n",
    "                        }\n",
    "                        save_to_jsonl(data)\n",
    "\n",
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 기존 파일이 있다면 삭제\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"기존 파일 '{OUTPUT_FILE}'을 삭제했습니다.\")\n",
    "        \n",
    "    print(\"FOMC 성명서 크롤링을 시작합니다.\")\n",
    "    print(f\"데이터 저장 위치: {OUTPUT_FILE}\")\n",
    "    print(f\"수집 대상 기간: {START_YEAR}년 ~ {END_YEAR}년\")\n",
    "    \n",
    "    # 2019년 말에 구조가 변경되었으므로 두 함수를 나누어 호출\n",
    "    crawl_historical_years(START_YEAR, 2019)\n",
    "    crawl_recent_years(2020, END_YEAR)\n",
    "    \n",
    "    print(\"--- 모든 크롤링 작업 완료 ---\")\n",
    "    print(f\"데이터가 '{OUTPUT_FILE}' 파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
