{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v6) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 10094.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 완료 ---\n",
      "중복 제거 후 1631개의 고유 문서를 확보했습니다.\n",
      "--- 4. 텍스트 정제 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1631 [12:38<64:13:33, 142.20s/it]"
     ]
    }
   ],
   "source": [
    "# SFT 코퍼스 클리닝 스크립트 (D1-D3)\n",
    "#\n",
    "# [2025-10-31 v6 수정]\n",
    "# - 'cleaned_text'가 비어있는 버그 수정\n",
    "# - 'Speeches' 파일의 본문을 통째로 삭제하던\n",
    "#   탐욕적(greedy) 정규식 `(r\"At the .*\", \"\")`를 삭제.\n",
    "# - 'Speeches' 머리말을 더 안전하게 제거하는 정규식으로 개선.\n",
    "#\n",
    "# [동작 방식]\n",
    "# 1. 'Downloads/statements' 폴더에서 모든 'fomc_*.jsonl' 파일을 로드합니다.\n",
    "# 2. 모든 데이터를 하나의 pandas DataFrame으로 병합합니다.\n",
    "# 3. [v4] 날짜(date) 컬럼을 표준 datetime 형식으로 정제 및 NaT 제거.\n",
    "# 4. URL 기준으로 중복 데이터를 제거합니다.\n",
    "# 5. [v6] 'text' 컬럼의 텍스트를 정제합니다 (헤더, 꼬리말, \\\\n 등 노이즈 제거).\n",
    "# 6. 최종 코퍼스를 'corpus.parquet' 파일로 저장합니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install -r requirements.txt`를 실행하여\n",
    "#    'pandas', 'pyarrow' 라이브러리를 설치하세요.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# tqdm이 pandas apply()와 잘 작동하도록 설정\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','data')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\") # (D1 Deliverable)\n",
    "\n",
    "# (v3) 텍스트 날짜 파싱을 위한 월(Month) 맵\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# [v6 수정] 정제(Cleaning)할 노이즈 패턴 (정규식)\n",
    "CLEANING_REGEX_PAIRS = [\n",
    "    # [v5] 텍스트로 포함된 줄바꿈 문자 (예: \"\\\\n\") 제거\n",
    "    (r'\\\\n', ' '), # 텍스트 '\\\\n' -> 공백\n",
    "    (r'\\\\r', ' '), # 텍스트 '\\\\r' -> 공백\n",
    "    (r'\\\\t', ' '), # 텍스트 '\\\\t' -> 공백\n",
    "    \n",
    "    # --- [v6] Speeches 머리말 정제 (안전한 방식) ---\n",
    "    # 예: \"...Kansas City, Missouri Share Watch Live ... Good morning\"\n",
    "    #     -> \"Good morning\"\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    # 'Watch Live'가 없는 구형 Speeches\n",
    "    # 예: \"...Washington, D.C. Thank you\"\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"), # 'At the'로 시작하는 경우\n",
    "    \n",
    "    # --- 웹사이트 공통 노이즈 ---\n",
    "    (r\"Home\\s*\\|\", \"\"),\n",
    "    (r\"News & Events\\s*\\|\", \"\"),\n",
    "    (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"),\n",
    "    (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"),\n",
    "    (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"),\n",
    "    (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"),\n",
    "    (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"),\n",
    "    (r\"Stay Connected\", \"\"),\n",
    "    (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"),\n",
    "    (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"),\n",
    "    (r\"Privacy Program\", \"\"),\n",
    "    (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"),\n",
    "    (r\"Office of Inspector General\", \"\"),\n",
    "    (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"),\n",
    "    (r\"Link to USA\\.gov\", \"\"),\n",
    "    (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"),\n",
    "    (r\"\\(HTML\\)\", \"\"),\n",
    "    (r\"Watch Live\", \"\"), # 'Share Watch Live'에서 놓친 나머지\n",
    "    (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"),\n",
    "    (r\"For immediate release\", \"\"),\n",
    "    (r\"FRB: Press Release --.*\", \"\"),\n",
    "    # --- Minutes/Speeches에서 발견되는 패턴 ---\n",
    "    (r\"Press Conference\", \"\"),\n",
    "    (r\"Projection Materials\", \"\"),\n",
    "    (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"),\n",
    "    # (r\"At the .*\", \"\"), # [v5 버그] -> [v6]에서 삭제됨\n",
    "    (r\"via prerecorded video\", \"\"),\n",
    "    (r\"\\(virtual\\)\", \"\"),\n",
    "    (r\"\\(via satellite\\)\", \"\"),\n",
    "    # --- 불필요한 공백 정제 ---\n",
    "    (r'\\s+', ' '), # 모든 실제 공백/줄바꿈을 1칸 공백으로\n",
    "]\n",
    "\n",
    "# --- 2. 헬퍼 함수 ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"정규식 패턴을 적용하여 텍스트를 정제합니다.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    cleaned = text\n",
    "    for pattern, replacement in compiled_regex:\n",
    "        cleaned = pattern.sub(replacement, cleaned)\n",
    "        \n",
    "    return cleaned.strip()\n",
    "\n",
    "# [v3] 정규식 패턴 미리 컴파일\n",
    "compiled_regex = []\n",
    "for pattern_str, replacement_str in CLEANING_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "# [v3 추가] 날짜 정제 함수 (D1)\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    \"2019-01-30\" (정상)과 \"February 1-2 Meeting - 2000\" (텍스트)를\n",
    "    모두 datetime 객체로 변환합니다.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "        \n",
    "    # 1. 'speeches', 'minutes' 파일처럼 이미 날짜 형식인 경우\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass # 실패하면 2단계로\n",
    "\n",
    "    # 2. 'statements' 파일의 텍스트 형식 (\"February 1-2 Meeting - 2000\")\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "        \n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "        \n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "            \n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1]) \n",
    "            \n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return None # 모든 파싱 실패\n",
    "\n",
    "# --- 3. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. 데이터 로드 (D1) ---\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"스크립트를 종료합니다.\")\n",
    "    else:\n",
    "        print(f\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v6) ---\") # v6\n",
    "        print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "        for f in jsonl_files:\n",
    "            print(f\"  - {os.path.basename(f)}\")\n",
    "        \n",
    "        df_list = []\n",
    "        for file_path in jsonl_files:\n",
    "            try:\n",
    "                df_part = pd.read_json(file_path, lines=True)\n",
    "                df_part['source_file'] = os.path.basename(file_path)\n",
    "                df_list.append(df_part)\n",
    "            except Exception as e:\n",
    "                print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "        if not df_list:\n",
    "            print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "            exit()\n",
    "            \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\n--- 1. 로드 완료 ---\")\n",
    "        print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "        \n",
    "        # --- 2. [v4] 날짜 정제 (D1) ---\n",
    "        print(f\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "        df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "        \n",
    "        df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "        \n",
    "        original_count_date = len(df)\n",
    "        df = df.dropna(subset=['datetime_clean'])\n",
    "        print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "        \n",
    "        df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "        # --- 3. 중복 제거 (D1) ---\n",
    "        df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "        df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "        \n",
    "        print(f\"--- 3. 중복 제거 완료 ---\")\n",
    "        print(f\"중복 제거 후 {len(df)}개의 고유 문서를 확보했습니다.\")\n",
    "\n",
    "        # --- 4. 텍스트 클리닝 (D1) ---\n",
    "        print(f\"--- 4. 텍스트 정제 시작 ---\")\n",
    "        if 'text' not in df.columns:\n",
    "            df['text'] = \"\"\n",
    "        df['text'] = df['text'].fillna(\"\")\n",
    "        \n",
    "        df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "        \n",
    "        original_count_text = len(df)\n",
    "        # [v6] 정제 후 100글자 미만의 너무 짧은 텍스트도 노이즈로 간주하고 제거\n",
    "        df = df[df['cleaned_text'].str.len() > 100]\n",
    "        print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "        # --- 5. 최종 저장 (D1-D3) ---\n",
    "        final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker', 'title', 'cleaned_text', 'url', 'source_file']\n",
    "        \n",
    "        for col in final_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        df_final = df[final_columns].rename(columns={\n",
    "            \"datetime_clean\": \"date\",\n",
    "            \"year_clean\": \"year\"\n",
    "        })\n",
    "        \n",
    "        df_final = df_final.sort_values(by='date')\n",
    "        \n",
    "        df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(f\"\\n--- 5. 저장 완료 ---\")\n",
    "        print(f\"최종 정제된 코퍼스(corpus)를 '{OUTPUT_FILE}' 파일에 저장했습니다.\")\n",
    "        print(f\"최종 문서 수: {len(df_final)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 10890.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 완료 ---\n",
      "중복 제거 후 1631개의 고유 문서를 확보했습니다.\n",
      "--- 4. 텍스트 정제 시작 ---\n",
      "정제 작업 중... (v7은 v6보다 다소 느릴 수 있으나, 멈춘 것이 아닙니다.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 811/1631 [2:30:01<5:00:24, 21.98s/it] "
     ]
    }
   ],
   "source": [
    "# SFT 코퍼스 클리닝 스크립트 (D1-D3)\n",
    "#\n",
    "# [2025-10-31 v7 수정]\n",
    "# - 14분에 5개 처리되는 \"catastrophic backtracking\" (정규식 성능 저하) 버그 수정\n",
    "# - CLEANING_REGEX를 'COMMON' (빠름)과 'SPEECH_HEADER' (느림)로 분리\n",
    "# - clean_text 함수가 'source_type'을 확인하여, 'Speech'가 아닐 경우\n",
    "#   느린 헤더 정규식을 건너뛰도록 수정\n",
    "# - .progress_apply(axis=1)을 사용하여 행(row) 단위로 적용\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install -r requirements.txt`를 실행하여\n",
    "#    'pandas', 'pyarrow' 라이브러리를 설치하세요.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# tqdm이 pandas apply()와 잘 작동하도록 설정\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','Data')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\") # (D1 Deliverable)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# [v7 수정] (1) 공통 정규식 (빠름, 모든 텍스트에 적용)\n",
    "COMMON_REGEX_PAIRS = [\n",
    "    (r'\\\\n', ' '), (r'\\\\r', ' '), (r'\\\\t', ' '),\n",
    "    (r\"Home\\s*\\|\", \"\"), (r\"News & Events\\s*\\|\", \"\"), (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"), (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"), (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"), (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"), (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"), (r\"Stay Connected\", \"\"), (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"), (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"), (r\"Privacy Program\", \"\"), (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"), (r\"Office of Inspector General\", \"\"), (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"), (r\"Link to USA\\.gov\", \"\"), (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"), (r\"\\(HTML\\)\", \"\"), (r\"Watch Live\", \"\"), (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"), (r\"For immediate release\", \"\"), (r\"FRB: Press Release --.*\", \"\"),\n",
    "    (r\"Press Conference\", \"\"), (r\"Projection Materials\", \"\"), (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"), (r\"via prerecorded video\", \"\"), (r\"\\(virtual\\)\", \"\"), (r\"\\(via satellite\\)\", \"\"),\n",
    "    (r'\\s+', ' '), # 맨 마지막에 공백 정제\n",
    "]\n",
    "\n",
    "# [v7 수정] (2) 연설문 헤더 정규식 (느림, 'Speeches'에만 적용)\n",
    "SPEECH_HEADER_REGEX_PAIRS = [\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"),\n",
    "]\n",
    "\n",
    "# --- 2. 헬퍼 함수 ---\n",
    "\n",
    "# [v7 수정] clean_text 함수가 'source_type'을 인자로 받음\n",
    "def clean_text(text, source_type):\n",
    "    \"\"\"정규식 패턴을 적용하여 텍스트를 정제합니다.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    cleaned = text\n",
    "    \n",
    "    # 1. 모든 문서에 공통 정규식 적용\n",
    "    for pattern, replacement in compiled_common_regex:\n",
    "        cleaned = pattern.sub(replacement, cleaned)\n",
    "        \n",
    "    # 2. [v7] 'Speech'인 경우에만 느린 헤더 정규식 추가 적용\n",
    "    if 'speech' in str(source_type).lower():\n",
    "        for pattern, replacement in compiled_speech_header_regex:\n",
    "            cleaned = pattern.sub(replacement, cleaned)\n",
    "            \n",
    "    return cleaned.strip()\n",
    "\n",
    "# [v7 수정] 정규식 2개 리스트로 컴파일\n",
    "compiled_common_regex = []\n",
    "for pattern_str, replacement_str in COMMON_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_common_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 공통 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "compiled_speech_header_regex = []\n",
    "for pattern_str, replacement_str in SPEECH_HEADER_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_speech_header_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 연설문 헤더 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    \"2019-01-30\" (정상)과 \"February 1-2 Meeting - 2000\" (텍스트)를\n",
    "    모두 datetime 객체로 변환합니다.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "        \n",
    "    # 1. 'speeches', 'minutes' 파일처럼 이미 날짜 형식인 경우\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass # 실패하면 2단계로\n",
    "\n",
    "    # 2. 'statements' 파일의 텍스트 형식 (\"February 1-2 Meeting - 2000\")\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "        \n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "        \n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "            \n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1]) \n",
    "            \n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return None # 모든 파싱 실패\n",
    "\n",
    "# --- 3. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. 데이터 로드 (D1) ---\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"스크립트를 종료합니다.\")\n",
    "    else:\n",
    "        print(f\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7) ---\") # v7\n",
    "        print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "        for f in jsonl_files:\n",
    "            print(f\"  - {os.path.basename(f)}\")\n",
    "        \n",
    "        df_list = []\n",
    "        for file_path in jsonl_files:\n",
    "            try:\n",
    "                df_part = pd.read_json(file_path, lines=True)\n",
    "                df_part['source_file'] = os.path.basename(file_path)\n",
    "                df_list.append(df_part)\n",
    "            except Exception as e:\n",
    "                print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "        if not df_list:\n",
    "            print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "            exit()\n",
    "            \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\n--- 1. 로드 완료 ---\")\n",
    "        print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "        \n",
    "        # --- 2. 날짜 정제 (D1) ---\n",
    "        print(f\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "        df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "        \n",
    "        df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "        \n",
    "        original_count_date = len(df)\n",
    "        df = df.dropna(subset=['datetime_clean'])\n",
    "        print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "        \n",
    "        df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "        # --- 3. 중복 제거 (D1) ---\n",
    "        df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "        df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "        \n",
    "        print(f\"--- 3. 중복 제거 완료 ---\")\n",
    "        print(f\"중복 제거 후 {len(df)}개의 고유 문서를 확보했습니다.\")\n",
    "\n",
    "        # --- 4. 텍스트 클리닝 (D1) ---\n",
    "        print(f\"--- 4. 텍스트 정제 시작 ---\")\n",
    "        # [v7] 'source_type'도 .fillna()\n",
    "        if 'text' not in df.columns: df['text'] = \"\"\n",
    "        if 'source_type' not in df.columns: df['source_type'] = \"\"\n",
    "        df['text'] = df['text'].fillna(\"\")\n",
    "        df['source_type'] = df['source_type'].fillna(\"\")\n",
    "        \n",
    "        # [v7 수정] .progress_apply()에서 lambda 함수로 source_type 전달 (axis=1)\n",
    "        print(\"정제 작업 중... (v7은 v6보다 다소 느릴 수 있으나, 멈춘 것이 아닙니다.)\")\n",
    "        df['cleaned_text'] = df.progress_apply(\n",
    "            lambda row: clean_text(row['text'], row['source_type']),\n",
    "            axis=1 # 행(row) 단위로 적용\n",
    "        )\n",
    "        \n",
    "        original_count_text = len(df)\n",
    "        df = df[df['cleaned_text'].str.len() > 100] # 100자 미만 제거\n",
    "        print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "        # --- 5. 최종 저장 (D1-D3) ---\n",
    "        final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker', 'title', 'cleaned_text', 'url', 'source_file']\n",
    "        \n",
    "        for col in final_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        df_final = df[final_columns].rename(columns={\n",
    "            \"datetime_clean\": \"date\",\n",
    "            \"year_clean\": \"year\"\n",
    "        })\n",
    "        \n",
    "        df_final = df_final.sort_values(by='date')\n",
    "        \n",
    "        df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(f\"\\n--- 5. 저장 완료 ---\")\n",
    "        print(f\"최종 정제된 코퍼스(corpus)를 '{OUTPUT_FILE}' 파일에 저장했습니다.\")\n",
    "        print(f\"최종 문서 수: {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972905a1",
   "metadata": {},
   "source": [
    "더 빠른 버전 이거 한번 돌려보기 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f58a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7-fast) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 80001.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 ---\n",
      "중복 제거 후 1647 → 1631개\n",
      "--- 4. 텍스트 정제 시작 (벡터화) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 후 비어있거나 너무 짧은(100자 미만) 행 51개를 삭제했습니다.\n",
      "--- 5. 저장 ---\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 248\u001b[39m\n\u001b[32m    242\u001b[39m df_final = df[final_columns].rename(columns={\n\u001b[32m    243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatetime_clean\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myear_clean\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m }).sort_values(by=\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# .parquet로 저장 (pyarrow 필요)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 완료 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    250\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m최종 정제된 코퍼스를 \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m에 저장했습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:3124\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3044\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3045\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyarrow\\types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SFT 코퍼스 클리닝 스크립트 (D1-D3)  [2025-10-31 v7-fast]\n",
    "- v7의 기능 유지 + 속도 최적화 (행 단위 apply 제거, 벡터화 클리닝)\n",
    "- 'Speeches'에만 느린 헤더 정규식 적용 (부분 시리즈 선택 + 트리거 프리필터)\n",
    "- 날짜 파싱은 안전성을 위해 progress_apply 유지\n",
    "\n",
    "[실행 전]\n",
    "pip install -r requirements.txt   # pandas, pyarrow, tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1) 설정 ---\n",
    "#DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements', 'Data')\n",
    "DOWNLOADS_DIR = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\Data\"\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\")  # (D1 Deliverable)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# --- 2) 정규식 패턴 정의 ---\n",
    "# [공통: 빠름] (모든 소스에 적용)\n",
    "COMMON_REGEX_PAIRS = [\n",
    "    (r'\\\\n', ' '), (r'\\\\r', ' '), (r'\\\\t', ' '),\n",
    "    (r\"Home\\s*\\|\", \"\"), (r\"News & Events\\s*\\|\", \"\"), (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"), (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"), (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"), (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"), (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"), (r\"Stay Connected\", \"\"), (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"), (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"), (r\"Privacy Program\", \"\"), (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"), (r\"Office of Inspector General\", \"\"), (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"), (r\"Link to USA\\.gov\", \"\"), (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"), (r\"\\(HTML\\)\", \"\"), (r\"Watch Live\", \"\"), (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"), (r\"For immediate release\", \"\"), (r\"FRB: Press Release --.*\", \"\"),\n",
    "    (r\"Press Conference\", \"\"), (r\"Projection Materials\", \"\"), (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"), (r\"via prerecorded video\", \"\"), (r\"\\(virtual\\)\", \"\"), (r\"\\(via satellite\\)\", \"\"),\n",
    "    (r'\\s+', ' ')  # 공백 정리 (추가로 마지막에 한 번 더 정리함)\n",
    "]\n",
    "\n",
    "# [스피치 헤더: 느림] (Speeches에만 적용)\n",
    "SPEECH_HEADER_REGEX_PAIRS = [\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"),\n",
    "]\n",
    "\n",
    "# --- 3) 정규식 컴파일 ---\n",
    "compiled_common_regex = []\n",
    "for pattern_str, replacement_str in COMMON_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_common_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 공통 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "compiled_speech_header_regex = []\n",
    "for pattern_str, replacement_str in SPEECH_HEADER_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_speech_header_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 연설문 헤더 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "\n",
    "# --- 4) 날짜 파싱 ---\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    '2019-01-30' 같은 ISO 날짜와\n",
    "    'February 1-2 Meeting - 2000' 같은 텍스트 모두를 datetime으로 변환.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "\n",
    "    # 1) 이미 날짜 포맷이면 우선 시도\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass  # 실패 시 2단계\n",
    "\n",
    "    # 2) 텍스트 포맷 해석\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "\n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "\n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "\n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            # \"1-2\" 같은 범위면 마지막 숫자를 사용\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1])\n",
    "\n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None  # 모두 실패\n",
    "\n",
    "\n",
    "# --- 5) 텍스트 클리닝 (벡터화 버전) ---\n",
    "def clean_text_vectorized(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    행 단위 apply(axis=1)를 쓰지 않고,\n",
    "    시리즈 단위로 공통 정규식 → (스피치만) 헤더 정규식 순서로 적용.\n",
    "    \"\"\"\n",
    "    # 안전한 기본 컬럼\n",
    "    if 'text' not in df.columns:\n",
    "        df['text'] = \"\"\n",
    "    if 'source_type' not in df.columns:\n",
    "        df['source_type'] = \"\"\n",
    "\n",
    "    s = df['text'].fillna(\"\").astype(str)\n",
    "    source_type = df['source_type'].fillna(\"\")\n",
    "\n",
    "    # 1) 값싼 전처리: 역슬래시 포함 제어문자 치환 (정규식 사용 안 함)\n",
    "    #    원문이 실제 개행(\\n)일 수도, '역슬래시+n' 문자열일 수도 있으므로 둘 다 정리\n",
    "    s = (s.str.replace('\\\\n', ' ', regex=False)\n",
    "           .str.replace('\\\\r', ' ', regex=False)\n",
    "           .str.replace('\\\\t', ' ', regex=False)\n",
    "           .str.replace('\\n', ' ', regex=False)\n",
    "           .str.replace('\\r', ' ', regex=False)\n",
    "           .str.replace('\\t', ' ', regex=False))\n",
    "\n",
    "    # 2) 공통 정규식 일괄 적용\n",
    "    for pattern, repl in compiled_common_regex:\n",
    "        s = s.str.replace(pattern, repl, regex=True)\n",
    "\n",
    "    # 3) 스피치만 헤더 정규식 적용 (부분 시리즈 선택 + 프리필터)\n",
    "    mask_speech = source_type.str.contains('speech', case=False, na=False)\n",
    "\n",
    "    # 헤더 트리거(없으면 아예 정규식 skip)\n",
    "    header_trigger = (\n",
    "        s.str.contains('Good morning|Good afternoon|Good evening', case=False, na=False) |\n",
    "        s.str.contains('Share Watch Live', case=False, na=False) |\n",
    "        s.str.match(r'(?i)At the .*?Thank you\\.', na=False)\n",
    "    )\n",
    "    mask_target = mask_speech & header_trigger\n",
    "\n",
    "    if mask_target.any():\n",
    "        s_sub = s[mask_target]\n",
    "        for pattern, repl in compiled_speech_header_regex:\n",
    "            s_sub = s_sub.str.replace(pattern, repl, regex=True)\n",
    "        s.loc[mask_target] = s_sub\n",
    "\n",
    "    # 4) 마지막 공백 정리 1회\n",
    "    s = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# --- 6) 메인 ---\n",
    "if __name__ == \"__main__\":\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "\n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    print(\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7-fast) ---\")\n",
    "    print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "    for f in jsonl_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    # 1) 로드\n",
    "    df_list = []\n",
    "    for file_path in jsonl_files:\n",
    "        try:\n",
    "            df_part = pd.read_json(file_path, lines=True)\n",
    "            df_part['source_file'] = os.path.basename(file_path)\n",
    "            df_list.append(df_part)\n",
    "        except Exception as e:\n",
    "            print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(\"\\n--- 1. 로드 완료 ---\")\n",
    "    print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "\n",
    "    # 2) 날짜 정제\n",
    "    print(\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = None\n",
    "\n",
    "    df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "    df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "\n",
    "    original_count_date = len(df)\n",
    "    df = df.dropna(subset=['datetime_clean'])\n",
    "    print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "    df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "    # 3) 중복 제거\n",
    "    print(\"--- 3. 중복 제거 ---\")\n",
    "    if 'url' not in df.columns:\n",
    "        df['url'] = None\n",
    "    if 'text' not in df.columns:\n",
    "        df['text'] = \"\"\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "    df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "    print(f\"중복 제거 후 {before} → {len(df)}개\")\n",
    "\n",
    "    # 4) 텍스트 클리닝 (벡터화)\n",
    "    print(\"--- 4. 텍스트 정제 시작 (벡터화) ---\")\n",
    "    df['cleaned_text'] = clean_text_vectorized(df)\n",
    "\n",
    "    original_count_text = len(df)\n",
    "    df = df[df['cleaned_text'].str.len() > 100]  # 100자 미만 제거\n",
    "    print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "    # 5) 최종 저장\n",
    "    print(\"--- 5. 저장 ---\")\n",
    "    final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker',\n",
    "                     'title', 'cleaned_text', 'url', 'source_file']\n",
    "\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df_final = df[final_columns].rename(columns={\n",
    "        \"datetime_clean\": \"date\",\n",
    "        \"year_clean\": \"year\"\n",
    "    }).sort_values(by='date')\n",
    "\n",
    "    # .parquet로 저장 (pyarrow 필요)\n",
    "    df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\n--- 완료 ---\")\n",
    "    print(f\"최종 정제된 코퍼스를 '{OUTPUT_FILE}'에 저장했습니다.\")\n",
    "    print(f\"최종 문서 수: {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d5b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: C:\\Users\\jeong\\Downloads\\statements\\statements\\corpus.parquet\n",
      "최종 문서 수: 1580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "OUTPUT_FILE = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\corpus.parquet\"\n",
    "\n",
    "# 혹시 pyarrow가 문제면 fastparquet로 강제 저장\n",
    "df_final.to_parquet(OUTPUT_FILE, index=False, engine=\"fastparquet\")\n",
    "\n",
    "print(\"✅ 저장 완료:\", OUTPUT_FILE)\n",
    "print(\"최종 문서 수:\", len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bc34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (22.0.0)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp313-cp313-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fastparquet) (2.3.4)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.11.0-cp313-cp313-win_amd64.whl.metadata (681 bytes)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading fastparquet-2024.11.0-cp313-cp313-win_amd64.whl (673 kB)\n",
      "   ---------------------------------------- 0.0/673.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 673.3/673.3 kB 6.0 MB/s  0:00:00\n",
      "Downloading cramjam-2.11.0-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 5.8 MB/s  0:00:00\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Installing collected packages: fsspec, cramjam, fastparquet\n",
      "\n",
      "   ------------- -------------------------- 1/3 [cramjam]\n",
      "   ---------------------------------------- 3/3 [fastparquet]\n",
      "\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.11.0 fsspec-2025.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U pyarrow fastparquet pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e003190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1580, 8)\n",
      "['date', 'year', 'source_type', 'speaker', 'title', 'cleaned_text', 'url', 'source_file']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\corpus.parquet\"\n",
    "\n",
    "df = pd.read_parquet(path, engine=\"fastparquet\")  # 엔진 강제\n",
    "\n",
    "print(\"shape:\", df.shape)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8dc5dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source_type</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>2015-05-04</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Opening Remarks</td>\n",
       "      <td>Governor Daniel K. Tarullo</td>\n",
       "      <td>May 04, 2015 Opening Remarks Governor Daniel K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2006-08-25</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Global Economic Integration: What's New and Wh...</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>August 25, 2006 Global Economic Integration: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Opening Remarks</td>\n",
       "      <td>Governor Michelle W. Bowman</td>\n",
       "      <td>July 24, 2024 Opening Remarks Governor Michell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>2025-06-18</td>\n",
       "      <td>FOMC Minutes</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FOMC FEDERAL RESERVE SYSTEM Minutes of the Jun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>2025-03-21</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Statement by Governor Christopher J. Waller</td>\n",
       "      <td>Governor Christopher J. Waller</td>\n",
       "      <td>March 21, 2025 Statement by Governor Christoph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   source_type  \\\n",
       "734  2015-05-04   FOMC Speech   \n",
       "109  2006-08-25   FOMC Speech   \n",
       "1430 2024-07-24   FOMC Speech   \n",
       "1532 2025-06-18  FOMC Minutes   \n",
       "1494 2025-03-21   FOMC Speech   \n",
       "\n",
       "                                                  title  \\\n",
       "734                                     Opening Remarks   \n",
       "109   Global Economic Integration: What's New and Wh...   \n",
       "1430                                    Opening Remarks   \n",
       "1532                                               None   \n",
       "1494        Statement by Governor Christopher J. Waller   \n",
       "\n",
       "                             speaker  \\\n",
       "734       Governor Daniel K. Tarullo   \n",
       "109         Chairman Ben S. Bernanke   \n",
       "1430     Governor Michelle W. Bowman   \n",
       "1532                            None   \n",
       "1494  Governor Christopher J. Waller   \n",
       "\n",
       "                                           cleaned_text  \n",
       "734   May 04, 2015 Opening Remarks Governor Daniel K...  \n",
       "109   August 25, 2006 Global Economic Integration: W...  \n",
       "1430  July 24, 2024 Opening Remarks Governor Michell...  \n",
       "1532  FOMC FEDERAL RESERVE SYSTEM Minutes of the Jun...  \n",
       "1494  March 21, 2025 Statement by Governor Christoph...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)[[\"date\", \"source_type\", \"title\", \"speaker\", \"cleaned_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b65ea2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source_type</th>\n",
       "      <th>speaker</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2007-11-06</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>Microfinance in the United States</td>\n",
       "      <td>November 06, 2007 Microfinance in the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>2013-12-16</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>Opening Remarks</td>\n",
       "      <td>December 16, 2013 Opening Remarks Chairman Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2007-03-21</td>\n",
       "      <td>FOMC Minutes (Historical)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Minutes of the March 20-21, 2007 A meeting of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                source_type                   speaker  \\\n",
       "219 2007-11-06                FOMC Speech  Chairman Ben S. Bernanke   \n",
       "652 2013-12-16                FOMC Speech  Chairman Ben S. Bernanke   \n",
       "154 2007-03-21  FOMC Minutes (Historical)                      None   \n",
       "\n",
       "                                 title  \\\n",
       "219  Microfinance in the United States   \n",
       "652                    Opening Remarks   \n",
       "154                               None   \n",
       "\n",
       "                                          cleaned_text  \n",
       "219  November 06, 2007 Microfinance in the United S...  \n",
       "652  December 16, 2013 Opening Remarks Chairman Ben...  \n",
       "154  Minutes of the March 20-21, 2007 A meeting of ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 source_type별 문서 수\n",
    "df[\"source_type\"].value_counts()\n",
    "\n",
    "# 텍스트 길이 통계\n",
    "df[\"len\"] = df[\"cleaned_text\"].str.len()\n",
    "df[\"len\"].describe()\n",
    "\n",
    "# 대표 샘플 몇 개\n",
    "df.sample(3)[[\"date\", \"source_type\", \"speaker\", \"title\", \"cleaned_text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67aa418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스 모양: (1580, 9)\n",
      "Index(['date', 'year', 'source_type', 'speaker', 'title', 'cleaned_text',\n",
      "       'url', 'source_file', 'len'],\n",
      "      dtype='object')\n",
      "세그먼트 모양: (237134, 8)\n",
      "   doc_id  sent_idx       date  year                source_type speaker title  \\\n",
      "0       0         0 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "1       0         1 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "2       0         2 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "3       0         3 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "4       0         4 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "\n",
      "                                                text  \n",
      "0  Minutes of the February 1-2, 2000 A meeting of...  \n",
      "1  Present: Mr. Greenspan, Chairman Mr. McDonough...  \n",
      "2  Moskow and Poole, Alternate Members of the Mes...  \n",
      "3  Boehne, McTeer, and Stern, Presidents of the F...  \n",
      "4  Eisenbeis, Goodfriend, Howard, Lindsey, Reinha...  \n",
      "✅ 저장 완료\n",
      " - parquet : C:\\Users\\jeong\\Downloads\\statements\\statements\\Data\\segments.parquet\n",
      " - jsonl   : C:\\Users\\jeong\\Downloads\\statements\\statements\\Data\\segments.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------\n",
    "# 0. (옵션) 만약 새 노트북에서 시작했다면 이렇게 다시 불러오기\n",
    "# corpus_path = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\corpus.parquet\"\n",
    "# df = pd.read_parquet(corpus_path, engine=\"fastparquet\")\n",
    "# ----------------------------------\n",
    "\n",
    "print(\"코퍼스 모양:\", df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "# 1. 간단한 문장 분리 함수 정의\n",
    "ABBR = r\"(Mr|Mrs|Ms|Dr|Prof|Sr|Jr|St|U\\.S|U\\.K|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|No)\"\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    \"\"\"영문 기준 간단한 문장 분리기\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # 공백 정리\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "    # 약어에 들어있는 마침표 보호\n",
    "    tmp = re.sub(rf\"\\b{ABBR}\\.\", lambda m: m.group(0).replace(\".\", \"<prd>\"), text)\n",
    "\n",
    "    # . ! ? 뒤 + 공백 + 대문자/숫자/따옴표 시작에서 분리\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+(?=[\\\"\\(A-Z0-9])\", tmp)\n",
    "\n",
    "    # 보호 마커 복원 + 앞뒤 공백 제거\n",
    "    sents = [p.replace(\"<prd>\", \".\").strip() for p in parts if p.strip()]\n",
    "    return sents\n",
    "\n",
    "# 2. 문장 단위로 펼치기\n",
    "rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    doc_id = idx  # 인덱스를 문서 ID로 사용 (필요하면 나중에 바꿔도 됨)\n",
    "    text = row[\"cleaned_text\"]\n",
    "    sents = split_sentences(text)\n",
    "\n",
    "    for si, sent in enumerate(sents):\n",
    "        rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sent_idx\": si,\n",
    "            \"date\": row.get(\"date\"),\n",
    "            \"year\": row.get(\"year\"),\n",
    "            \"source_type\": row.get(\"source_type\"),\n",
    "            \"speaker\": row.get(\"speaker\"),\n",
    "            \"title\": row.get(\"title\"),\n",
    "            \"text\": sent,\n",
    "        })\n",
    "\n",
    "segments = pd.DataFrame(rows)\n",
    "print(\"세그먼트 모양:\", segments.shape)\n",
    "print(segments.head())\n",
    "\n",
    "# 3. 저장 경로 설정\n",
    "OUT_DIR = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\Data\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "parquet_path = os.path.join(OUT_DIR, \"segments.parquet\")\n",
    "jsonl_path   = os.path.join(OUT_DIR, \"segments.jsonl\")\n",
    "\n",
    "# 4. 저장 (parquet + jsonl 둘 다)\n",
    "segments.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "segments.to_json(jsonl_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"✅ 저장 완료\")\n",
    "print(\" - parquet :\", parquet_path)\n",
    "print(\" - jsonl   :\", jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28bf9a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source_type</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139186</th>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>43</td>\n",
       "      <td>But such disturbances are not a great concern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169928</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>FOMC Minutes</td>\n",
       "      <td>35</td>\n",
       "      <td>Gust, Deputy Associate Director, Division of M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72868</th>\n",
       "      <td>2011-06-09</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>61</td>\n",
       "      <td>The greater decline in house prices in low- an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139456</th>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>313</td>\n",
       "      <td>Relatedly, Yoon, Kim, and Lee (2014) and Jusel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92259</th>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>FOMC Speech</td>\n",
       "      <td>120</td>\n",
       "      <td>Too-Big-to-Fail.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date   source_type  sent_idx  \\\n",
       "139186 2017-09-26   FOMC Speech        43   \n",
       "169928 2020-09-16  FOMC Minutes        35   \n",
       "72868  2011-06-09   FOMC Speech        61   \n",
       "139456 2017-09-26   FOMC Speech       313   \n",
       "92259  2013-05-03   FOMC Speech       120   \n",
       "\n",
       "                                                     text  \n",
       "139186  But such disturbances are not a great concern ...  \n",
       "169928  Gust, Deputy Associate Director, Division of M...  \n",
       "72868   The greater decline in house prices in low- an...  \n",
       "139456  Relatedly, Yoon, Kim, and Lee (2014) and Jusel...  \n",
       "92259                                    Too-Big-to-Fail.  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments.sample(5)[[\"date\", \"source_type\", \"sent_idx\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be68e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/28.0 MB 3.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.8/28.0 MB 2.2 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 1.6/28.0 MB 2.7 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.4/28.0 MB 3.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 3.1/28.0 MB 3.2 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 3.9/28.0 MB 3.3 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.7/28.0 MB 3.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.5/28.0 MB 3.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 6.3/28.0 MB 3.6 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 7.3/28.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 8.1/28.0 MB 3.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.9/28.0 MB 3.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.2/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 11.0/28.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 3.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.6/28.0 MB 3.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 13.6/28.0 MB 4.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 14.4/28.0 MB 4.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 14.9/28.0 MB 4.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 16.0/28.0 MB 4.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 17.0/28.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 18.1/28.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.1/28.0 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 20.2/28.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.2/28.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.3/28.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.3/28.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.4/28.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.7/28.0 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 4.4 MB/s  0:00:06\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa00ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797b48a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 6.2 MB/s  0:00:01\n",
      "Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 6.7 MB/s  0:00:01\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.4 pandas-2.3.3 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40ccd0",
   "metadata": {},
   "source": [
    "5. Testbed 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66c7f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 시나리오 개수: 48\n",
      "저장: C:\\Users\\jeong\\Downloads\\statements\\statements\\testbed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "inflations_real = [1.0, 2.0, 3.0, 4.0]\n",
    "inflations_ext  = [6.0, 8.0]\n",
    "unemps_real     = [3.0, 4.0, 5.0]\n",
    "unemps_ext      = [2.0, 8.0]\n",
    "growths         = [0.5, 1.5, 2.5]\n",
    "\n",
    "def make_templates():\n",
    "    return [\n",
    "        (\"You are a member of the FOMC. \"\n",
    "         \"Inflation is {inflation}% and unemployment is {unemployment}%. \"\n",
    "         \"Answer with one word: raise, hold, or cut.\"),\n",
    "        (\"You are setting the federal funds rate. \"\n",
    "         \"Inflation: {inflation}%, unemployment: {unemployment}%, \"\n",
    "         \"GDP growth: {growth}%. \"\n",
    "         \"Respond with exactly one token: raise / hold / cut.\"),\n",
    "        (\"Consider an economy with {inflation}% inflation and \"\n",
    "         \"{unemployment}% unemployment. \"\n",
    "         \"What should the Fed do with the policy rate? \"\n",
    "         \"Reply: raise, hold, or cut.\")\n",
    "    ]\n",
    "\n",
    "scenarios = []\n",
    "\n",
    "# realistic region\n",
    "for inf, un, gr in itertools.product(inflations_real, unemps_real, growths):\n",
    "    sid = f\"real_inf{inf}_un{un}_gr{gr}\"\n",
    "    scenarios.append({\n",
    "        \"scenario_id\": sid,\n",
    "        \"region\": \"realistic\",\n",
    "        \"state\": {\"inflation\": inf, \"unemployment\": un, \"growth\": gr},\n",
    "        \"templates\": make_templates(),\n",
    "    })\n",
    "\n",
    "# stress region\n",
    "for inf, un, gr in itertools.product(inflations_ext, unemps_ext, growths):\n",
    "    sid = f\"stress_inf{inf}_un{un}_gr{gr}\"\n",
    "    scenarios.append({\n",
    "        \"scenario_id\": sid,\n",
    "        \"region\": \"stress\",\n",
    "        \"state\": {\"inflation\": inf, \"unemployment\": un, \"growth\": gr},\n",
    "        \"templates\": make_templates(),\n",
    "    })\n",
    "\n",
    "out_path = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\testbed.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sc in scenarios:\n",
    "        f.write(json.dumps(sc) + \"\\n\")\n",
    "\n",
    "print(\"총 시나리오 개수:\", len(scenarios))\n",
    "print(\"저장:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27760b",
   "metadata": {},
   "source": [
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8714028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
