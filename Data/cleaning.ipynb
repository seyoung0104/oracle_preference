{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v6) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 10094.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 완료 ---\n",
      "중복 제거 후 1631개의 고유 문서를 확보했습니다.\n",
      "--- 4. 텍스트 정제 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1631 [12:38<64:13:33, 142.20s/it]"
     ]
    }
   ],
   "source": [
    "# SFT 코퍼스 클리닝 스크립트 (D1-D3)\n",
    "#\n",
    "# [2025-10-31 v6 수정]\n",
    "# - 'cleaned_text'가 비어있는 버그 수정\n",
    "# - 'Speeches' 파일의 본문을 통째로 삭제하던\n",
    "#   탐욕적(greedy) 정규식 `(r\"At the .*\", \"\")`를 삭제.\n",
    "# - 'Speeches' 머리말을 더 안전하게 제거하는 정규식으로 개선.\n",
    "#\n",
    "# [동작 방식]\n",
    "# 1. 'Downloads/statements' 폴더에서 모든 'fomc_*.jsonl' 파일을 로드합니다.\n",
    "# 2. 모든 데이터를 하나의 pandas DataFrame으로 병합합니다.\n",
    "# 3. [v4] 날짜(date) 컬럼을 표준 datetime 형식으로 정제 및 NaT 제거.\n",
    "# 4. URL 기준으로 중복 데이터를 제거합니다.\n",
    "# 5. [v6] 'text' 컬럼의 텍스트를 정제합니다 (헤더, 꼬리말, \\\\n 등 노이즈 제거).\n",
    "# 6. 최종 코퍼스를 'corpus.parquet' 파일로 저장합니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install -r requirements.txt`를 실행하여\n",
    "#    'pandas', 'pyarrow' 라이브러리를 설치하세요.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# tqdm이 pandas apply()와 잘 작동하도록 설정\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','data')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\") # (D1 Deliverable)\n",
    "\n",
    "# (v3) 텍스트 날짜 파싱을 위한 월(Month) 맵\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# [v6 수정] 정제(Cleaning)할 노이즈 패턴 (정규식)\n",
    "CLEANING_REGEX_PAIRS = [\n",
    "    # [v5] 텍스트로 포함된 줄바꿈 문자 (예: \"\\\\n\") 제거\n",
    "    (r'\\\\n', ' '), # 텍스트 '\\\\n' -> 공백\n",
    "    (r'\\\\r', ' '), # 텍스트 '\\\\r' -> 공백\n",
    "    (r'\\\\t', ' '), # 텍스트 '\\\\t' -> 공백\n",
    "    \n",
    "    # --- [v6] Speeches 머리말 정제 (안전한 방식) ---\n",
    "    # 예: \"...Kansas City, Missouri Share Watch Live ... Good morning\"\n",
    "    #     -> \"Good morning\"\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    # 'Watch Live'가 없는 구형 Speeches\n",
    "    # 예: \"...Washington, D.C. Thank you\"\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"), # 'At the'로 시작하는 경우\n",
    "    \n",
    "    # --- 웹사이트 공통 노이즈 ---\n",
    "    (r\"Home\\s*\\|\", \"\"),\n",
    "    (r\"News & Events\\s*\\|\", \"\"),\n",
    "    (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"),\n",
    "    (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"),\n",
    "    (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"),\n",
    "    (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"),\n",
    "    (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"),\n",
    "    (r\"Stay Connected\", \"\"),\n",
    "    (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"),\n",
    "    (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"),\n",
    "    (r\"Privacy Program\", \"\"),\n",
    "    (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"),\n",
    "    (r\"Office of Inspector General\", \"\"),\n",
    "    (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"),\n",
    "    (r\"Link to USA\\.gov\", \"\"),\n",
    "    (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"),\n",
    "    (r\"\\(HTML\\)\", \"\"),\n",
    "    (r\"Watch Live\", \"\"), # 'Share Watch Live'에서 놓친 나머지\n",
    "    (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"),\n",
    "    (r\"For immediate release\", \"\"),\n",
    "    (r\"FRB: Press Release --.*\", \"\"),\n",
    "    # --- Minutes/Speeches에서 발견되는 패턴 ---\n",
    "    (r\"Press Conference\", \"\"),\n",
    "    (r\"Projection Materials\", \"\"),\n",
    "    (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"),\n",
    "    # (r\"At the .*\", \"\"), # [v5 버그] -> [v6]에서 삭제됨\n",
    "    (r\"via prerecorded video\", \"\"),\n",
    "    (r\"\\(virtual\\)\", \"\"),\n",
    "    (r\"\\(via satellite\\)\", \"\"),\n",
    "    # --- 불필요한 공백 정제 ---\n",
    "    (r'\\s+', ' '), # 모든 실제 공백/줄바꿈을 1칸 공백으로\n",
    "]\n",
    "\n",
    "# --- 2. 헬퍼 함수 ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"정규식 패턴을 적용하여 텍스트를 정제합니다.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    cleaned = text\n",
    "    for pattern, replacement in compiled_regex:\n",
    "        cleaned = pattern.sub(replacement, cleaned)\n",
    "        \n",
    "    return cleaned.strip()\n",
    "\n",
    "# [v3] 정규식 패턴 미리 컴파일\n",
    "compiled_regex = []\n",
    "for pattern_str, replacement_str in CLEANING_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "# [v3 추가] 날짜 정제 함수 (D1)\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    \"2019-01-30\" (정상)과 \"February 1-2 Meeting - 2000\" (텍스트)를\n",
    "    모두 datetime 객체로 변환합니다.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "        \n",
    "    # 1. 'speeches', 'minutes' 파일처럼 이미 날짜 형식인 경우\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass # 실패하면 2단계로\n",
    "\n",
    "    # 2. 'statements' 파일의 텍스트 형식 (\"February 1-2 Meeting - 2000\")\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "        \n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "        \n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "            \n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1]) \n",
    "            \n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return None # 모든 파싱 실패\n",
    "\n",
    "# --- 3. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. 데이터 로드 (D1) ---\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"스크립트를 종료합니다.\")\n",
    "    else:\n",
    "        print(f\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v6) ---\") # v6\n",
    "        print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "        for f in jsonl_files:\n",
    "            print(f\"  - {os.path.basename(f)}\")\n",
    "        \n",
    "        df_list = []\n",
    "        for file_path in jsonl_files:\n",
    "            try:\n",
    "                df_part = pd.read_json(file_path, lines=True)\n",
    "                df_part['source_file'] = os.path.basename(file_path)\n",
    "                df_list.append(df_part)\n",
    "            except Exception as e:\n",
    "                print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "        if not df_list:\n",
    "            print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "            exit()\n",
    "            \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\n--- 1. 로드 완료 ---\")\n",
    "        print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "        \n",
    "        # --- 2. [v4] 날짜 정제 (D1) ---\n",
    "        print(f\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "        df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "        \n",
    "        df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "        \n",
    "        original_count_date = len(df)\n",
    "        df = df.dropna(subset=['datetime_clean'])\n",
    "        print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "        \n",
    "        df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "        # --- 3. 중복 제거 (D1) ---\n",
    "        df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "        df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "        \n",
    "        print(f\"--- 3. 중복 제거 완료 ---\")\n",
    "        print(f\"중복 제거 후 {len(df)}개의 고유 문서를 확보했습니다.\")\n",
    "\n",
    "        # --- 4. 텍스트 클리닝 (D1) ---\n",
    "        print(f\"--- 4. 텍스트 정제 시작 ---\")\n",
    "        if 'text' not in df.columns:\n",
    "            df['text'] = \"\"\n",
    "        df['text'] = df['text'].fillna(\"\")\n",
    "        \n",
    "        df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "        \n",
    "        original_count_text = len(df)\n",
    "        # [v6] 정제 후 100글자 미만의 너무 짧은 텍스트도 노이즈로 간주하고 제거\n",
    "        df = df[df['cleaned_text'].str.len() > 100]\n",
    "        print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "        # --- 5. 최종 저장 (D1-D3) ---\n",
    "        final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker', 'title', 'cleaned_text', 'url', 'source_file']\n",
    "        \n",
    "        for col in final_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        df_final = df[final_columns].rename(columns={\n",
    "            \"datetime_clean\": \"date\",\n",
    "            \"year_clean\": \"year\"\n",
    "        })\n",
    "        \n",
    "        df_final = df_final.sort_values(by='date')\n",
    "        \n",
    "        df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(f\"\\n--- 5. 저장 완료 ---\")\n",
    "        print(f\"최종 정제된 코퍼스(corpus)를 '{OUTPUT_FILE}' 파일에 저장했습니다.\")\n",
    "        print(f\"최종 문서 수: {len(df_final)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 10890.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 완료 ---\n",
      "중복 제거 후 1631개의 고유 문서를 확보했습니다.\n",
      "--- 4. 텍스트 정제 시작 ---\n",
      "정제 작업 중... (v7은 v6보다 다소 느릴 수 있으나, 멈춘 것이 아닙니다.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 811/1631 [2:30:01<5:00:24, 21.98s/it] "
     ]
    }
   ],
   "source": [
    "# SFT 코퍼스 클리닝 스크립트 (D1-D3)\n",
    "#\n",
    "# [2025-10-31 v7 수정]\n",
    "# - 14분에 5개 처리되는 \"catastrophic backtracking\" (정규식 성능 저하) 버그 수정\n",
    "# - CLEANING_REGEX를 'COMMON' (빠름)과 'SPEECH_HEADER' (느림)로 분리\n",
    "# - clean_text 함수가 'source_type'을 확인하여, 'Speech'가 아닐 경우\n",
    "#   느린 헤더 정규식을 건너뛰도록 수정\n",
    "# - .progress_apply(axis=1)을 사용하여 행(row) 단위로 적용\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install -r requirements.txt`를 실행하여\n",
    "#    'pandas', 'pyarrow' 라이브러리를 설치하세요.\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# tqdm이 pandas apply()와 잘 작동하도록 설정\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','Data')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\") # (D1 Deliverable)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# [v7 수정] (1) 공통 정규식 (빠름, 모든 텍스트에 적용)\n",
    "COMMON_REGEX_PAIRS = [\n",
    "    (r'\\\\n', ' '), (r'\\\\r', ' '), (r'\\\\t', ' '),\n",
    "    (r\"Home\\s*\\|\", \"\"), (r\"News & Events\\s*\\|\", \"\"), (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"), (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"), (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"), (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"), (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"), (r\"Stay Connected\", \"\"), (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"), (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"), (r\"Privacy Program\", \"\"), (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"), (r\"Office of Inspector General\", \"\"), (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"), (r\"Link to USA\\.gov\", \"\"), (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"), (r\"\\(HTML\\)\", \"\"), (r\"Watch Live\", \"\"), (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"), (r\"For immediate release\", \"\"), (r\"FRB: Press Release --.*\", \"\"),\n",
    "    (r\"Press Conference\", \"\"), (r\"Projection Materials\", \"\"), (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"), (r\"via prerecorded video\", \"\"), (r\"\\(virtual\\)\", \"\"), (r\"\\(via satellite\\)\", \"\"),\n",
    "    (r'\\s+', ' '), # 맨 마지막에 공백 정제\n",
    "]\n",
    "\n",
    "# [v7 수정] (2) 연설문 헤더 정규식 (느림, 'Speeches'에만 적용)\n",
    "SPEECH_HEADER_REGEX_PAIRS = [\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"),\n",
    "]\n",
    "\n",
    "# --- 2. 헬퍼 함수 ---\n",
    "\n",
    "# [v7 수정] clean_text 함수가 'source_type'을 인자로 받음\n",
    "def clean_text(text, source_type):\n",
    "    \"\"\"정규식 패턴을 적용하여 텍스트를 정제합니다.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    cleaned = text\n",
    "    \n",
    "    # 1. 모든 문서에 공통 정규식 적용\n",
    "    for pattern, replacement in compiled_common_regex:\n",
    "        cleaned = pattern.sub(replacement, cleaned)\n",
    "        \n",
    "    # 2. [v7] 'Speech'인 경우에만 느린 헤더 정규식 추가 적용\n",
    "    if 'speech' in str(source_type).lower():\n",
    "        for pattern, replacement in compiled_speech_header_regex:\n",
    "            cleaned = pattern.sub(replacement, cleaned)\n",
    "            \n",
    "    return cleaned.strip()\n",
    "\n",
    "# [v7 수정] 정규식 2개 리스트로 컴파일\n",
    "compiled_common_regex = []\n",
    "for pattern_str, replacement_str in COMMON_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_common_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 공통 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "compiled_speech_header_regex = []\n",
    "for pattern_str, replacement_str in SPEECH_HEADER_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_speech_header_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 연설문 헤더 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    \"2019-01-30\" (정상)과 \"February 1-2 Meeting - 2000\" (텍스트)를\n",
    "    모두 datetime 객체로 변환합니다.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "        \n",
    "    # 1. 'speeches', 'minutes' 파일처럼 이미 날짜 형식인 경우\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass # 실패하면 2단계로\n",
    "\n",
    "    # 2. 'statements' 파일의 텍스트 형식 (\"February 1-2 Meeting - 2000\")\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "        \n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "        \n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "            \n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1]) \n",
    "            \n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return None # 모든 파싱 실패\n",
    "\n",
    "# --- 3. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. 데이터 로드 (D1) ---\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"스크립트를 종료합니다.\")\n",
    "    else:\n",
    "        print(f\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7) ---\") # v7\n",
    "        print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "        for f in jsonl_files:\n",
    "            print(f\"  - {os.path.basename(f)}\")\n",
    "        \n",
    "        df_list = []\n",
    "        for file_path in jsonl_files:\n",
    "            try:\n",
    "                df_part = pd.read_json(file_path, lines=True)\n",
    "                df_part['source_file'] = os.path.basename(file_path)\n",
    "                df_list.append(df_part)\n",
    "            except Exception as e:\n",
    "                print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "        if not df_list:\n",
    "            print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "            exit()\n",
    "            \n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"\\n--- 1. 로드 완료 ---\")\n",
    "        print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "        \n",
    "        # --- 2. 날짜 정제 (D1) ---\n",
    "        print(f\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "        df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "        \n",
    "        df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "        \n",
    "        original_count_date = len(df)\n",
    "        df = df.dropna(subset=['datetime_clean'])\n",
    "        print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "        \n",
    "        df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "        # --- 3. 중복 제거 (D1) ---\n",
    "        df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "        df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "        \n",
    "        print(f\"--- 3. 중복 제거 완료 ---\")\n",
    "        print(f\"중복 제거 후 {len(df)}개의 고유 문서를 확보했습니다.\")\n",
    "\n",
    "        # --- 4. 텍스트 클리닝 (D1) ---\n",
    "        print(f\"--- 4. 텍스트 정제 시작 ---\")\n",
    "        # [v7] 'source_type'도 .fillna()\n",
    "        if 'text' not in df.columns: df['text'] = \"\"\n",
    "        if 'source_type' not in df.columns: df['source_type'] = \"\"\n",
    "        df['text'] = df['text'].fillna(\"\")\n",
    "        df['source_type'] = df['source_type'].fillna(\"\")\n",
    "        \n",
    "        # [v7 수정] .progress_apply()에서 lambda 함수로 source_type 전달 (axis=1)\n",
    "        print(\"정제 작업 중... (v7은 v6보다 다소 느릴 수 있으나, 멈춘 것이 아닙니다.)\")\n",
    "        df['cleaned_text'] = df.progress_apply(\n",
    "            lambda row: clean_text(row['text'], row['source_type']),\n",
    "            axis=1 # 행(row) 단위로 적용\n",
    "        )\n",
    "        \n",
    "        original_count_text = len(df)\n",
    "        df = df[df['cleaned_text'].str.len() > 100] # 100자 미만 제거\n",
    "        print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "        # --- 5. 최종 저장 (D1-D3) ---\n",
    "        final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker', 'title', 'cleaned_text', 'url', 'source_file']\n",
    "        \n",
    "        for col in final_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        df_final = df[final_columns].rename(columns={\n",
    "            \"datetime_clean\": \"date\",\n",
    "            \"year_clean\": \"year\"\n",
    "        })\n",
    "        \n",
    "        df_final = df_final.sort_values(by='date')\n",
    "        \n",
    "        df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(f\"\\n--- 5. 저장 완료 ---\")\n",
    "        print(f\"최종 정제된 코퍼스(corpus)를 '{OUTPUT_FILE}' 파일에 저장했습니다.\")\n",
    "        print(f\"최종 문서 수: {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972905a1",
   "metadata": {},
   "source": [
    "더 빠른 버전 이거 한번 돌려보기 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58a184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7-fast) ---\n",
      "총 4개의 .jsonl 파일을 찾았습니다:\n",
      "  - fomc_minutes_2000-2019.jsonl\n",
      "  - fomc_minutes_2020-present.jsonl\n",
      "  - fomc_speeches_2000-present.jsonl\n",
      "  - fomc_statements_2000-present.jsonl\n",
      "\n",
      "--- 1. 로드 완료 ---\n",
      "총 1651개의 원본 문서를 로드했습니다.\n",
      "--- 2. 날짜(date) 컬럼 표준화 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1651/1651 [00:00<00:00, 52101.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜 파싱 실패/누락 행 4개를 삭제했습니다.\n",
      "--- 3. 중복 제거 ---\n",
      "중복 제거 후 1647 → 1631개\n",
      "--- 4. 텍스트 정제 시작 (벡터화) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SFT 코퍼스 클리닝 스크립트 (D1-D3)  [2025-10-31 v7-fast]\n",
    "- v7의 기능 유지 + 속도 최적화 (행 단위 apply 제거, 벡터화 클리닝)\n",
    "- 'Speeches'에만 느린 헤더 정규식 적용 (부분 시리즈 선택 + 트리거 프리필터)\n",
    "- 날짜 파싱은 안전성을 위해 progress_apply 유지\n",
    "\n",
    "[실행 전]\n",
    "pip install -r requirements.txt   # pandas, pyarrow, tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1) 설정 ---\n",
    "#DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements', 'Data')\n",
    "DOWNLOADS_DIR = r\"C:\\Users\\jeong\\Downloads\\statements\\statements\\Data\"\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\")  # (D1 Deliverable)\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "# --- 2) 정규식 패턴 정의 ---\n",
    "# [공통: 빠름] (모든 소스에 적용)\n",
    "COMMON_REGEX_PAIRS = [\n",
    "    (r'\\\\n', ' '), (r'\\\\r', ' '), (r'\\\\t', ' '),\n",
    "    (r\"Home\\s*\\|\", \"\"), (r\"News & Events\\s*\\|\", \"\"), (r\"Monetary Policy\\s*\\|\", \"\"),\n",
    "    (r\"About the Fed\\s*\\|\", \"\"), (r\"Board of Governors of the Federal Reserve System\", \"\"),\n",
    "    (r\"Federal Open Market Committee\", \"\"), (r\"Skip to main content\", \"\"),\n",
    "    (r\"Last Update:.*\", \"\"), (r\"An official website of the United States Government\", \"\"),\n",
    "    (r\"Here's how you know\", \"\"), (r\"Search\\s*Submit Search Button\", \"\"),\n",
    "    (r\"Back to Top\", \"\"), (r\"Stay Connected\", \"\"), (r\"Tools and Information\", \"\"),\n",
    "    (r\"Contact\\s*\\|\\s*Publications\\s*\\|\", \"\"), (r\"Freedom of Information \\(FOIA\\)\", \"\"),\n",
    "    (r\"Accessibility\", \"\"), (r\"Privacy Program\", \"\"), (r\"Website Policies\", \"\"),\n",
    "    (r\"Español\", \"\"), (r\"Office of Inspector General\", \"\"), (r\"Budget & Performance\", \"\"),\n",
    "    (r\"No FEAR Act\", \"\"), (r\"Link to USA\\.gov\", \"\"), (r\"Link to Open\\.gov\", \"\"),\n",
    "    (r\"\\(PDF\\)\", \"\"), (r\"\\(HTML\\)\", \"\"), (r\"Watch Live\", \"\"), (r\"Implementation Note\", \"\"),\n",
    "    (r\"Release Date:.*\", \"\"), (r\"For immediate release\", \"\"), (r\"FRB: Press Release --.*\", \"\"),\n",
    "    (r\"Press Conference\", \"\"), (r\"Projection Materials\", \"\"), (r\"\\(Released.*\\)\", \"\"),\n",
    "    (r\"Listen\", \"\"), (r\"via prerecorded video\", \"\"), (r\"\\(virtual\\)\", \"\"), (r\"\\(via satellite\\)\", \"\"),\n",
    "    (r'\\s+', ' ')  # 공백 정리 (추가로 마지막에 한 번 더 정리함)\n",
    "]\n",
    "\n",
    "# [스피치 헤더: 느림] (Speeches에만 적용)\n",
    "SPEECH_HEADER_REGEX_PAIRS = [\n",
    "    (r\".*Share Watch Live.*?Good morning\", \"Good morning\"),\n",
    "    (r\".*Share Watch Live.*?Good afternoon\", \"Good afternoon\"),\n",
    "    (r\".*Share Watch Live.*?Good evening\", \"Good evening\"),\n",
    "    (r\"^At the .*?Thank you\\. \", \"Thank you. \"),\n",
    "]\n",
    "\n",
    "# --- 3) 정규식 컴파일 ---\n",
    "compiled_common_regex = []\n",
    "for pattern_str, replacement_str in COMMON_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_common_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 공통 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "compiled_speech_header_regex = []\n",
    "for pattern_str, replacement_str in SPEECH_HEADER_REGEX_PAIRS:\n",
    "    try:\n",
    "        compiled_speech_header_regex.append((re.compile(pattern_str, re.IGNORECASE), replacement_str))\n",
    "    except re.error as e:\n",
    "        print(f\"[경고] 연설문 헤더 정규식 컴파일 오류: '{pattern_str}' ({e})\")\n",
    "\n",
    "\n",
    "# --- 4) 날짜 파싱 ---\n",
    "def parse_messy_date(date_obj):\n",
    "    \"\"\"\n",
    "    '2019-01-30' 같은 ISO 날짜와\n",
    "    'February 1-2 Meeting - 2000' 같은 텍스트 모두를 datetime으로 변환.\n",
    "    \"\"\"\n",
    "    if pd.isna(date_obj):\n",
    "        return None\n",
    "\n",
    "    # 1) 이미 날짜 포맷이면 우선 시도\n",
    "    try:\n",
    "        parsed_date = pd.to_datetime(date_obj, errors='coerce')\n",
    "        if not pd.isna(parsed_date):\n",
    "            return parsed_date\n",
    "    except Exception:\n",
    "        pass  # 실패 시 2단계\n",
    "\n",
    "    # 2) 텍스트 포맷 해석\n",
    "    try:\n",
    "        date_str = str(date_obj)\n",
    "\n",
    "        month_day_match = re.search(r'([A-Za-z]+)\\s+([\\d-]+)', date_str)\n",
    "        year_match = re.search(r'(\\d{4})', date_str)\n",
    "\n",
    "        if month_day_match and year_match:\n",
    "            month_str = month_day_match.group(1)\n",
    "            day_str = month_day_match.group(2)\n",
    "            year_int = int(year_match.group(1))\n",
    "\n",
    "            month_int = MONTH_MAP.get(month_str)\n",
    "            # \"1-2\" 같은 범위면 마지막 숫자를 사용\n",
    "            day_int = int(re.split(r'[-/]', day_str)[-1])\n",
    "\n",
    "            if month_int:\n",
    "                return datetime.datetime(year_int, month_int, day_int)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None  # 모두 실패\n",
    "\n",
    "\n",
    "# --- 5) 텍스트 클리닝 (벡터화 버전) ---\n",
    "def clean_text_vectorized(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    행 단위 apply(axis=1)를 쓰지 않고,\n",
    "    시리즈 단위로 공통 정규식 → (스피치만) 헤더 정규식 순서로 적용.\n",
    "    \"\"\"\n",
    "    # 안전한 기본 컬럼\n",
    "    if 'text' not in df.columns:\n",
    "        df['text'] = \"\"\n",
    "    if 'source_type' not in df.columns:\n",
    "        df['source_type'] = \"\"\n",
    "\n",
    "    s = df['text'].fillna(\"\").astype(str)\n",
    "    source_type = df['source_type'].fillna(\"\")\n",
    "\n",
    "    # 1) 값싼 전처리: 역슬래시 포함 제어문자 치환 (정규식 사용 안 함)\n",
    "    #    원문이 실제 개행(\\n)일 수도, '역슬래시+n' 문자열일 수도 있으므로 둘 다 정리\n",
    "    s = (s.str.replace('\\\\n', ' ', regex=False)\n",
    "           .str.replace('\\\\r', ' ', regex=False)\n",
    "           .str.replace('\\\\t', ' ', regex=False)\n",
    "           .str.replace('\\n', ' ', regex=False)\n",
    "           .str.replace('\\r', ' ', regex=False)\n",
    "           .str.replace('\\t', ' ', regex=False))\n",
    "\n",
    "    # 2) 공통 정규식 일괄 적용\n",
    "    for pattern, repl in compiled_common_regex:\n",
    "        s = s.str.replace(pattern, repl, regex=True)\n",
    "\n",
    "    # 3) 스피치만 헤더 정규식 적용 (부분 시리즈 선택 + 프리필터)\n",
    "    mask_speech = source_type.str.contains('speech', case=False, na=False)\n",
    "\n",
    "    # 헤더 트리거(없으면 아예 정규식 skip)\n",
    "    header_trigger = (\n",
    "        s.str.contains('Good morning|Good afternoon|Good evening', case=False, na=False) |\n",
    "        s.str.contains('Share Watch Live', case=False, na=False) |\n",
    "        s.str.match(r'(?i)At the .*?Thank you\\.', na=False)\n",
    "    )\n",
    "    mask_target = mask_speech & header_trigger\n",
    "\n",
    "    if mask_target.any():\n",
    "        s_sub = s[mask_target]\n",
    "        for pattern, repl in compiled_speech_header_regex:\n",
    "            s_sub = s_sub.str.replace(pattern, repl, regex=True)\n",
    "        s.loc[mask_target] = s_sub\n",
    "\n",
    "    # 4) 마지막 공백 정리 1회\n",
    "    s = s.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# --- 6) 메인 ---\n",
    "if __name__ == \"__main__\":\n",
    "    jsonl_files = glob.glob(os.path.join(DOWNLOADS_DIR, \"fomc_*.jsonl\"))\n",
    "\n",
    "    if not jsonl_files:\n",
    "        print(f\"[오류] '{DOWNLOADS_DIR}'에서 'fomc_*.jsonl' 파일을 찾을 수 없습니다.\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    print(\"--- SFT 코퍼스 클리닝 시작 (D1-D3) (v7-fast) ---\")\n",
    "    print(f\"총 {len(jsonl_files)}개의 .jsonl 파일을 찾았습니다:\")\n",
    "    for f in jsonl_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    # 1) 로드\n",
    "    df_list = []\n",
    "    for file_path in jsonl_files:\n",
    "        try:\n",
    "            df_part = pd.read_json(file_path, lines=True)\n",
    "            df_part['source_file'] = os.path.basename(file_path)\n",
    "            df_list.append(df_part)\n",
    "        except Exception as e:\n",
    "            print(f\"  [경고] '{file_path}' 파일 로드 실패: {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"[오류] 로드할 수 있는 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(\"\\n--- 1. 로드 완료 ---\")\n",
    "    print(f\"총 {len(df)}개의 원본 문서를 로드했습니다.\")\n",
    "\n",
    "    # 2) 날짜 정제\n",
    "    print(\"--- 2. 날짜(date) 컬럼 표준화 시작 ---\")\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = None\n",
    "\n",
    "    df['datetime_clean'] = df['date'].progress_apply(parse_messy_date)\n",
    "    df['datetime_clean'] = pd.to_datetime(df['datetime_clean'], errors='coerce')\n",
    "\n",
    "    original_count_date = len(df)\n",
    "    df = df.dropna(subset=['datetime_clean'])\n",
    "    print(f\"날짜 파싱 실패/누락 행 {original_count_date - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "    df['year_clean'] = df['datetime_clean'].dt.year\n",
    "\n",
    "    # 3) 중복 제거\n",
    "    print(\"--- 3. 중복 제거 ---\")\n",
    "    if 'url' not in df.columns:\n",
    "        df['url'] = None\n",
    "    if 'text' not in df.columns:\n",
    "        df['text'] = \"\"\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['url']).drop_duplicates(subset=['url'])\n",
    "    df = df.dropna(subset=['text']).drop_duplicates(subset=['text'])\n",
    "    print(f\"중복 제거 후 {before} → {len(df)}개\")\n",
    "\n",
    "    # 4) 텍스트 클리닝 (벡터화)\n",
    "    print(\"--- 4. 텍스트 정제 시작 (벡터화) ---\")\n",
    "    df['cleaned_text'] = clean_text_vectorized(df)\n",
    "\n",
    "    original_count_text = len(df)\n",
    "    df = df[df['cleaned_text'].str.len() > 100]  # 100자 미만 제거\n",
    "    print(f\"정제 후 비어있거나 너무 짧은(100자 미만) 행 {original_count_text - len(df)}개를 삭제했습니다.\")\n",
    "\n",
    "    # 5) 최종 저장\n",
    "    print(\"--- 5. 저장 ---\")\n",
    "    final_columns = ['datetime_clean', 'year_clean', 'source_type', 'speaker',\n",
    "                     'title', 'cleaned_text', 'url', 'source_file']\n",
    "\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df_final = df[final_columns].rename(columns={\n",
    "        \"datetime_clean\": \"date\",\n",
    "        \"year_clean\": \"year\"\n",
    "    }).sort_values(by='date')\n",
    "\n",
    "    # .parquet로 저장 (pyarrow 필요)\n",
    "    df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\n--- 완료 ---\")\n",
    "    print(f\"최종 정제된 코퍼스를 '{OUTPUT_FILE}'에 저장했습니다.\")\n",
    "    print(f\"최종 문서 수: {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa00ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797b48a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jeong\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.0 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 6.2 MB/s  0:00:01\n",
      "Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 6.7 MB/s  0:00:01\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.4 pandas-2.3.3 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\jeong\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8595817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'corpus.parquet' 파일 분석 시작 ---\n",
      "\n",
      "[ 1. DataFrame 정보 (df.info()) ]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1631 entries, 0 to 1630\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1631 non-null   datetime64[ns]\n",
      " 1   year          1631 non-null   int64         \n",
      " 2   source_type   1631 non-null   object        \n",
      " 3   speaker       1263 non-null   object        \n",
      " 4   title         1263 non-null   object        \n",
      " 5   cleaned_text  1631 non-null   object        \n",
      " 6   url           1631 non-null   object        \n",
      " 7   source_file   1631 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(6)\n",
      "memory usage: 102.1+ KB\n",
      "\n",
      "\n",
      "[ 2. 데이터 상위 5개 (df.head()) ]\n",
      "        date  year                source_type speaker title  \\\n",
      "0 2000-02-02  2000             FOMC Statement    None  None   \n",
      "1 2000-02-02  2000  FOMC Minutes (Historical)    None  None   \n",
      "2 2000-03-21  2000  FOMC Minutes (Historical)    None  None   \n",
      "3 2000-03-21  2000             FOMC Statement    None  None   \n",
      "4 2000-05-16  2000  FOMC Minutes (Historical)    None  None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          cleaned_text  \\\n",
      "0  The voted today to raise its target for the federal funds rate by 25 basis points to 5-3/4 percent. In a related action, the Board of Governors approved a 25 basis point increase in the discount rate to 5-1/4 percent. The Committee remains concerned that over time increases in demand will continue to exceed the growth in potential supply, even after taking account of the pronounced rise in productivity growth. Such trends could foster inflationary imbalances that would undermine the economy'...   \n",
      "1  Minutes of the February 1-2, 2000 A meeting of the was held in the offices of the in Washington, D.C., on Tuesday, February 1, 2000, at 2:30 p.m. and continued on Wednesday, February 2, 2000, at 9:00 a.m. Present: Mr. Greenspan, Chairman Mr. McDonough, Vice Chairman Mr. Broaddus Mr. Ferguson Mr. Gramlich Mr. Guynn Mr. Jordan Mr. Kelley Mr. Meyer Mr. Parry Mr. Hoenig, Ms. Minehan, Messrs. Moskow and Poole, Alternate Members of the Messrs. Boehne, McTeer, and Stern, Presidents of the Federal R...   \n",
      "2  Minutes of the March 21, 2000 A meeting of the was held in the offices of the in Washington, D.C., on Tuesday, March 21, 2000, at 9:00 a.m. Present: Mr. Greenspan, Chairman Mr. McDonough, Vice Chairman Mr. Broaddus Mr. Ferguson Mr. Gramlich Mr. Guynn Mr. Jordan Mr. Kelley Mr. Meyer Mr. Parry Mr. Hoenig, Ms. Minehan, Messrs. Moskow, Poole, and Stewart, Alternate Members of the Messrs. Boehne, McTeer, and Stern, Presidents of the Federal Reserve Banks of Philadelphia, Dallas, and Minneapolis r...   \n",
      "3  The voted today to raise its target for the federal funds rate by 25 basis points to 6 percent. In a related action, the Board of Governors approved a 25 basis point increase in the discount rate to 5-1/2 percent. Economic conditions and considerations addressed by the Committee are essentially the same as when the Committee met in February. The Committee remains concerned that increases in demand will continue to exceed the growth in potential supply, which could foster inflationary imbalan...   \n",
      "4  Minutes of the May 16, 2000 A meeting of the was held in the offices of the in Washington, D.C., on Tuesday, May 16, 2000, at 9:00 a.m. Present: Mr. Greenspan, Chairman Mr. McDonough, Vice Chairman Mr. Broaddus Mr. Ferguson Mr. Gramlich Mr. Guynn Mr. Jordan Mr. Kelley Mr. Meyer Mr. Parry Mr. Hoenig, Ms. Minehan, Messrs. Moskow and Poole, Alternate Members of the Messrs. McTeer and Stern, Presidents of the Federal Reserve Banks of Dallas and Minneapolis respectively Mr. Kohn, Secretary and Ec...   \n",
      "\n",
      "                                                                     url  \\\n",
      "0  https://www.federalreserve.gov/boarddocs/press/general/2000/20000202/   \n",
      "1               https://www.federalreserve.gov/fomc/minutes/20000202.htm   \n",
      "2               https://www.federalreserve.gov/fomc/minutes/20000321.htm   \n",
      "3  https://www.federalreserve.gov/boarddocs/press/general/2000/20000321/   \n",
      "4               https://www.federalreserve.gov/fomc/minutes/20000516.htm   \n",
      "\n",
      "                          source_file  \n",
      "0  fomc_statements_2000-present.jsonl  \n",
      "1        fomc_minutes_2000-2019.jsonl  \n",
      "2        fomc_minutes_2000-2019.jsonl  \n",
      "3  fomc_statements_2000-present.jsonl  \n",
      "4        fomc_minutes_2000-2019.jsonl  \n",
      "\n",
      "\n",
      "[ 3. 데이터 소스별 개수 (value_counts()) ]\n",
      "FOMC Speech                  1263\n",
      "FOMC Statement                166\n",
      "FOMC Minutes (Historical)     156\n",
      "FOMC Minutes                   46\n",
      "Name: source_type, dtype: int64\n",
      "\n",
      "\n",
      "[ 4. 정제된 텍스트(cleaned_text) 샘플 3개 ]\n",
      "676                                                                                                                                                                                                                                                                                                                                                                                                                                     June 27, 2013 Thoughts on Unconventional Monetary Policy Governor Jerome H. Powell\n",
      "293                                                                                                                                                                                                                                                                                                                                                                                                             February 27, 2008 The Importance of Economic Education and Financial Literacy Governor Frederic S. Mishkin\n",
      "835    Information received since the met in December suggests that labor market conditions improved further even as economic growth slowed late last year. Household spending and business fixed investment have been increasing at moderate rates in recent months, and the housing sector has improved further; however, net exports have been soft and inventory investment slowed. A range of recent labor market indicators, including strong job gains, points to some additional decline in underutilization of...\n",
      "Name: cleaned_text, dtype: object\n",
      "\n",
      "--- 분석 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# corpus.parquet 파일의 내용을 확인하기 위한 스크립트\n",
    "#\n",
    "# [동작 방식]\n",
    "# 1. 'corpus.parquet' 파일을 pandas DataFrame으로 로드합니다.\n",
    "# 2. df.info() - 컬럼별 타입, 누락된 값(non-null) 개수를 출력합니다.\n",
    "# 3. df.head() - 상위 5개 행을 출력합니다.\n",
    "# 4. df['source_type'].value_counts() - 수집된 데이터 소스별 개수를 출력합니다.\n",
    "# 5. 정제된 텍스트 샘플 3개를 출력합니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 'pandas', 'pyarrow'가 설치되어 있어야 합니다.\n",
    "#    (clean_corpus.py를 실행했다면 이미 설치되어 있습니다.)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','Data')\n",
    "CORPUS_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\")\n",
    "\n",
    "# --- 2. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        print(f\"[오류] '{CORPUS_FILE}'을 찾을 수 없습니다.\")\n",
    "        print(\"먼저 `clean_corpus.py` 스크립트를 실행하여 'corpus.parquet' 파일을 생성해야 합니다.\")\n",
    "    else:\n",
    "        print(f\"--- 'corpus.parquet' 파일 분석 시작 ---\")\n",
    "        \n",
    "        # 1. Parquet 파일 로드\n",
    "        df = pd.read_parquet(CORPUS_FILE)\n",
    "        \n",
    "        print(f\"\\n[ 1. DataFrame 정보 (df.info()) ]\")\n",
    "        # .info()는 컬럼명, 데이터 개수, 데이터 타입을 보여줍니다.\n",
    "        df.info()\n",
    "        \n",
    "        print(f\"\\n\\n[ 2. 데이터 상위 5개 (df.head()) ]\")\n",
    "        # .head()는 상위 5개 행을 보여줍니다.\n",
    "        # (date, year, speaker, title 등이 잘 파싱되었는지 확인)\n",
    "        print(df.head())\n",
    "        \n",
    "        print(f\"\\n\\n[ 3. 데이터 소스별 개수 (value_counts()) ]\")\n",
    "        # (D1) 데이터가 균형있게 수집되었는지 확인\n",
    "        if 'source_type' in df.columns:\n",
    "            print(df['source_type'].value_counts())\n",
    "        else:\n",
    "            print(\"'source_type' 컬럼을 찾을 수 없습니다.\")\n",
    "            \n",
    "        print(f\"\\n\\n[ 4. 정제된 텍스트(cleaned_text) 샘플 3개 ]\")\n",
    "        # (D1) 텍스트가 깨끗하게 정제되었는지 (꼬리말/메뉴 제거) 확인\n",
    "        if 'cleaned_text' in df.columns and len(df) >= 3:\n",
    "            pd.set_option('display.max_colwidth', 500) # 텍스트가 잘리지 않게 설정\n",
    "            print(df.sample(n=3)['cleaned_text'])\n",
    "        else:\n",
    "            print(\"'cleaned_text' 컬럼이 없거나 데이터가 3개 미만입니다.\")\n",
    "            \n",
    "        print(f\"\\n--- 분석 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd30bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 'corpus.parquet' 파일 분석 시작 ---\n",
      "\n",
      "\n",
      "[ 4. 정제된 텍스트(cleaned_text) 샘플 20개 ]\n",
      "1437                                                                                                                                                                                                                                                                                                                                                                                                                                                                  April 03, 2024 Opening Remarks Chair Jerome H. Powell\n",
      "1212                                                                                                                                                                                                                                                                                                                                                                                                                                  June 23, 2021 Building Economic Resilience in Communities Governor Michelle W. Bowman\n",
      "272                                                                                                                                                                                                                                                                                                                                                                                                                               November 13, 2007 Implementing Basel II in the United States Governor Randall S. Kroszner\n",
      "1370                                                                                                                                                                                                                                                                                                                                                                                                                    October 02, 2023 Monetary Policy and Financial Stability Vice Chair for Supervision Michael S. Barr\n",
      "239     The decided today to keep its target for the federal funds rate at 5-1/4 percent. Economic growth was moderate during the first half of the year. Financial markets have been volatile in recent weeks, credit conditions have become tighter for some households and businesses, and the housing correction is ongoing. Nevertheless, the economy seems likely to continue to expand at a moderate pace over coming quarters, supported by solid growth in employment and incomes and a robust global economy. ...\n",
      "828                                                                                                                                                                                                                                                                                                                                                                                                              December 01, 2015 Normalizing Monetary Policy When the Neutral Interest Rate Is Low Governor Lael Brainard\n",
      "1221                                                                                                                                                                                                                                                                                                                                                                                                                                            August 27, 2021 Monetary Policy in the Time of COVID Chair Jerome H. Powell\n",
      "1266                                                                                                                                                                                                                                                                                                                                                                                                                                                June 03, 2022 Risk in the Crypto Markets Governor Christopher J. Waller\n",
      "1159    October 15, 2020 Modernizing and Strengthening CRA Regulations: A Conversation with Minority Depository Institutions Governor Lael Brainard To the National Bankers Association (via webcast) Share I want to thank Kim Saunders and Kenneth Kelly for inviting me to join members of the National Bankers Association (NBA) for a conversation today. 1 As mission-driven financial institutions with a focus on serving minority households and businesses, you see the significant racial disparities in our ...\n",
      "393     Information received since the met in March indicates th In light of increasing economic slack here and abroad, the Committee expects that inflation will remain subdued. Moreover, the Committee sees some risk that inflation could persist for a time below rates that best foster economic growth and price stability in the longer term. In these circumstances, the Federal Reserve will employ all available tools to promote economic recovery and to preserve price stability. The Committee will maint...\n",
      "754                                                                                                                                                                                                                                                                                                                                                                                                                                                                October 30, 2014 Welcoming Remarks Chair Janet L. Yellen\n",
      "808                                                                                                                                                                                                                                                                                                                                                                                                                        September 28, 2015 Capital Regulation Across Financial Intermediaries Governor Daniel K. Tarullo\n",
      "731                                                                                                                                                                                                                                                                                                                                                                                                                                        June 06, 2014 A Conversation on Central Banking Issues Governor Jerome H. Powell\n",
      "1510                                                                                                                                                                                                                                                                                                                                                                                                                                                           October 23, 2024 Opening Remarks Governor Michelle W. Bowman\n",
      "895                                                                                                                                                                                                                                                                                                                                                                                                                                                                January 12, 2017 Welcoming Remarks Chair Janet L. Yellen\n",
      "23      Minutes of the May 15, 2001 A meeting of the was held in the offices of the in Washington, D.C., on Tuesday, May 15, 2001, starting at 9:00 a.m. Present: Mr. Greenspan, Chairman Mr. McDonough, Vice Chairman Mr. Ferguson Mr. Gramlich Mr. Hoenig Mr. Kelley Mr. Meyer Ms. Minehan Mr. Moskow Mr. Poole Messrs. Jordan, McTeer, Santomero, and Stern, Alternate Members of the Messrs. Broaddus, Guynn, and Parry, Presidents of the Federal Reserve Banks of Richmond, Atlanta, and San Francisco respectivel...\n",
      "1405                                                                                                                                                                                                                                                                                                                                                                                                     December 01, 2023 The Importance of Effective Liquidity Risk Management Vice Chair for Supervision Michael S. Barr\n",
      "595                                                                                                                                                                                                                                                                                                                                                                                                                                                               March 14, 2012 Community Banking Chairman Ben S. Bernanke\n",
      "896                                                                                                                                                                                                                                                                                                                                                                                                                                        January 17, 2017 Monetary Policy in a Time of Uncertainty Governor Lael Brainard\n",
      "984     May 25, 2018 Financial Stability and Central Bank Transparency Chairman Jerome H. Powell At \"350 years of Central Banking: The Past, the Present and the Future,\" A Sveriges Riksbank anniversary conference sponsored by the Riksbank and the Riksdag, Stockholm, Sweden Share Thank you for inviting me here to celebrate this important milestone. Today is a special day for all of us, since the founding of the Riksbank 350 years ago marked the beginning of central banking. 1 As we meet to discuss th...\n",
      "Name: cleaned_text, dtype: object\n",
      "\n",
      "--- 분석 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# corpus.parquet 파일의 내용을 확인하기 위한 스크립트\n",
    "#\n",
    "# [동작 방식]\n",
    "# 1. 'corpus.parquet' 파일을 pandas DataFrame으로 로드합니다.\n",
    "# 2. df.info() - 컬럼별 타입, 누락된 값(non-null) 개수를 출력합니다.\n",
    "# 3. df.head() - 상위 5개 행을 출력합니다.\n",
    "# 4. df['source_type'].value_counts() - 수집된 데이터 소스별 개수를 출력합니다.\n",
    "# 5. 정제된 텍스트 샘플 3개를 출력합니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 'pandas', 'pyarrow'가 설치되어 있어야 합니다.\n",
    "#    (clean_corpus.py를 실행했다면 이미 설치되어 있습니다.)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements','Data')\n",
    "CORPUS_FILE = os.path.join(DOWNLOADS_DIR, \"corpus.parquet\")\n",
    "\n",
    "# --- 2. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        print(f\"[오류] '{CORPUS_FILE}'을 찾을 수 없습니다.\")\n",
    "        print(\"먼저 `clean_corpus.py` 스크립트를 실행하여 'corpus.parquet' 파일을 생성해야 합니다.\")\n",
    "    else:\n",
    "        print(f\"--- 'corpus.parquet' 파일 분석 시작 ---\")\n",
    "        \n",
    "        # 1. Parquet 파일 로드\n",
    "        df = pd.read_parquet(CORPUS_FILE)\n",
    "        \n",
    "        # print(f\"\\n[ 1. DataFrame 정보 (df.info()) ]\")\n",
    "        # .info()는 컬럼명, 데이터 개수, 데이터 타입을 보여줍니다.\n",
    "        # df.info()\n",
    "        \n",
    "        # print(f\"\\n\\n[ 2. 데이터 상위 5개 (df.head()) ]\")\n",
    "        # .head()는 상위 5개 행을 보여줍니다.\n",
    "        # (date, year, speaker, title 등이 잘 파싱되었는지 확인)\n",
    "        # print(df.head())\n",
    "        \n",
    "        # print(f\"\\n\\n[ 3. 데이터 소스별 개수 (value_counts()) ]\")\n",
    "        # (D1) 데이터가 균형있게 수집되었는지 확인\n",
    "        # if 'source_type' in df.columns:\n",
    "        #     print(df['source_type'].value_counts())\n",
    "        # else:\n",
    "        #     print(\"'source_type' 컬럼을 찾을 수 없습니다.\")\n",
    "            \n",
    "        print(f\"\\n\\n[ 4. 정제된 텍스트(cleaned_text) 샘플 20개 ]\")\n",
    "        # (D1) 텍스트가 깨끗하게 정제되었는지 (꼬리말/메뉴 제거) 확인\n",
    "        if 'cleaned_text' in df.columns and len(df) >= 20:\n",
    "            pd.set_option('display.max_colwidth', 500) # 텍스트가 잘리지 않게 설정\n",
    "            print(df.sample(n=20)['cleaned_text'])\n",
    "        elif 'cleaned_text' in df.columns:\n",
    "            pd.set_option('display.max_colwidth', 500) # 텍스트가 잘리지 않게 설정\n",
    "            print(df.sample(n=len(df))['cleaned_text'])\n",
    "        else:\n",
    "            print(\"'cleaned_text' 컬럼이 없거나 데이터가 3개 미만입니다.\")\n",
    "            \n",
    "        print(f\"\\n--- 분석 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c6d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Missing file: fomc_statements_2000-present.jsonl\n",
      "[WARN] Missing file: fomc_minutes_2000-2019.jsonl\n",
      "[WARN] Missing file: fomc_minutes_2020-present.jsonl\n",
      "[WARN] Missing file: fomc_speeches_2000-present.jsonl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24720\\2421749448.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;31m# CSV로 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;31m# ===============================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m \u001b[0munified\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"doc_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"first\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0munified_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBASE\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"unified_docs.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82109\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82109\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6304\u001b[1;33m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6306\u001b[0m             \u001b[1;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82109\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   6302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6304\u001b[1;33m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6306\u001b[0m             \u001b[1;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\82109\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1838\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1840\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# STEP 1: FOMC JSONL 통합 → 통합 CSV 생성 (Python 3.9 호환)\n",
    "# ===============================================================\n",
    "\n",
    "import json, re, hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# 🔹 1. 폴더 설정 (필요시 수정)\n",
    "BASE = Path(\"/statements/Data\")  # 👉 네 JSONL 파일이 있는 폴더 경로\n",
    "\n",
    "# 🔹 2. 입력 파일 목록\n",
    "INPUTS = [\n",
    "    (\"fomc_statements_2000-present.jsonl\", \"statements\"),\n",
    "    (\"fomc_minutes_2000-2019.jsonl\", \"minutes\"),\n",
    "    (\"fomc_minutes_2020-present.jsonl\", \"minutes\"),\n",
    "    (\"fomc_speeches_2000-present.jsonl\", \"speeches\"),\n",
    "]\n",
    "\n",
    "# ===============================================================\n",
    "# 헬퍼 함수들\n",
    "# ===============================================================\n",
    "\n",
    "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"JSONL 파일을 안전하게 읽어서 리스트로 반환\"\"\"\n",
    "    rows = []\n",
    "    if not path.exists():\n",
    "        print(f\"[WARN] Missing file: {path.name}\")\n",
    "        return rows\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    rows.append(json.loads(line.rstrip(\",\")))\n",
    "                except Exception as e2:\n",
    "                    rows.append({\"__parse_error__\": str(e2), \"__raw__\": line, \"__line__\": i})\n",
    "    return rows\n",
    "\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"공백·개행 정규화\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\xa0\", \" \").replace(\"\\u200b\", \"\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def robust_parse_date(val: Any) -> Optional[datetime]:\n",
    "    \"\"\"다양한 날짜 포맷 자동 인식\"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, (int, float)):\n",
    "        try:\n",
    "            return datetime.fromtimestamp(float(val))\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        fmts = [\n",
    "            \"%Y-%m-%d\", \"%Y/%m/%d\",\n",
    "            \"%b %d, %Y\", \"%B %d, %Y\",\n",
    "            \"%d %b %Y\", \"%d %B %Y\",\n",
    "            \"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            \"%m/%d/%Y\"\n",
    "        ]\n",
    "        for fmt in fmts:\n",
    "            try:\n",
    "                return datetime.strptime(s, fmt)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # fallback: YYYY-MM-DD 패턴 추출\n",
    "        m = re.match(r\"(\\d{4}-\\d{2}-\\d{2})\", s)\n",
    "        if m:\n",
    "            try:\n",
    "                return datetime.strptime(m.group(1), \"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_doc_id(text: str, date: Optional[datetime], source: str, title: str) -> str:\n",
    "    \"\"\"문서 고유 해시 ID 생성\"\"\"\n",
    "    key = f\"{source}|{title}|{date.isoformat() if isinstance(date, datetime) else ''}|{(text or '')[:2000]}\"\n",
    "    return hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "# ===============================================================\n",
    "# 메인 처리\n",
    "# ===============================================================\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "for fname, source in INPUTS:\n",
    "    path = BASE / fname\n",
    "    data = load_jsonl(path)\n",
    "\n",
    "    for r in data:\n",
    "        # 파싱 실패 라인 스킵\n",
    "        if \"__parse_error__\" in r and all(k not in r for k in (\"text\",\"content\",\"body\",\"raw_text\",\"article\",\"transcript\")):\n",
    "            continue\n",
    "\n",
    "        # 키 통합\n",
    "        text = r.get(\"text\") or r.get(\"content\") or r.get(\"body\") or r.get(\"raw_text\") or r.get(\"article\") or r.get(\"transcript\") or \"\"\n",
    "        title = r.get(\"title\") or r.get(\"heading\") or r.get(\"name\") or \"\"\n",
    "        date_raw = r.get(\"date\") or r.get(\"published\") or r.get(\"time\") or r.get(\"timestamp\")\n",
    "        url = r.get(\"url\") or r.get(\"link\") or r.get(\"source_url\")\n",
    "        speaker = r.get(\"speaker\") or r.get(\"author\") or r.get(\"presenter\")\n",
    "        meeting_id = r.get(\"meeting_id\") or r.get(\"meetingId\") or r.get(\"meeting\")\n",
    "\n",
    "        dt = robust_parse_date(date_raw)\n",
    "        text_norm = normalize_whitespace(text)\n",
    "        if not text_norm:\n",
    "            continue\n",
    "\n",
    "        if not meeting_id and isinstance(dt, datetime):\n",
    "            meeting_id = dt.strftime(\"%Y%m%d\")\n",
    "\n",
    "        doc_id = make_doc_id(text_norm, dt, source, title or \"\")\n",
    "\n",
    "        rows_out.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source\": source,\n",
    "            \"title\": title or \"\",\n",
    "            \"date\": dt.date().isoformat() if isinstance(dt, datetime) else None,\n",
    "            \"year\": dt.year if isinstance(dt, datetime) else None,\n",
    "            \"url\": url or \"\",\n",
    "            \"speaker\": speaker or \"\",\n",
    "            \"meeting_id\": str(meeting_id) if meeting_id else \"\",\n",
    "            \"text\": text_norm,\n",
    "            \"char_len\": len(text_norm),\n",
    "        })\n",
    "\n",
    "# ===============================================================\n",
    "# CSV로 저장\n",
    "# ===============================================================\n",
    "unified = pd.DataFrame(rows_out).sort_values([\"date\",\"source\",\"doc_id\"], na_position=\"first\").reset_index(drop=True)\n",
    "\n",
    "unified_path = BASE / \"unified_docs.csv\"\n",
    "manifest_path = BASE / \"data_manifest.csv\"\n",
    "\n",
    "unified.to_csv(unified_path, index=False)\n",
    "unified[[\"doc_id\",\"source\",\"date\",\"year\",\"title\",\"url\",\"speaker\",\"meeting_id\",\"char_len\"]].to_csv(manifest_path, index=False)\n",
    "\n",
    "print(f\"[✅ 완료] {len(unified)}개의 문서를 통합 저장했습니다.\")\n",
    "print(f\"- 통합 문서: {unified_path}\")\n",
    "print(f\"- 메타 데이터: {manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7f14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
