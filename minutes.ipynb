{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e64a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디버깅용 HTML 파일 수집 시작...\n",
      "URL: https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\n",
      "\n",
      "--- 성공 ---\n",
      "HTML 내용을 'C:\\Users\\82109\\Downloads\\statements\\debug_calendar.html' 파일로 저장했습니다.\n",
      "이 파일을 열어서 내용을 확인하거나, 저에게 다시 공유해주세요.\n"
     ]
    }
   ],
   "source": [
    "# 디버깅용 스크립트\n",
    "#\n",
    "# focc_minutes.py가 0개를 수집하는 문제를 해결하기 위해,\n",
    "# 캘린더 페이지의 현재 HTML 구조를 파일로 저장합니다.\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# --- 설정 ---\n",
    "CALENDAR_URL = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "# 저장할 경로\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "DEBUG_FILE = os.path.join(DOWNLOADS_DIR, \"debug_calendar.html\")\n",
    "\n",
    "print(f\"디버깅용 HTML 파일 수집 시작...\")\n",
    "print(f\"URL: {CALENDAR_URL}\")\n",
    "\n",
    "try:\n",
    "    # 폴더 생성 확인\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "    \n",
    "    # --- HTML 요청 ---\n",
    "    response = requests.get(CALENDAR_URL, headers=HEADERS)\n",
    "    response.raise_for_status() # 오류가 있으면 예외 발생\n",
    "    \n",
    "    # --- 파일로 저장 ---\n",
    "    with open(DEBUG_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "        \n",
    "    print(f\"\\n--- 성공 ---\")\n",
    "    print(f\"HTML 내용을 '{DEBUG_FILE}' 파일로 저장했습니다.\")\n",
    "    print(\"이 파일을 열어서 내용을 확인하거나, 저에게 다시 공유해주세요.\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n--- 오류 ---\")\n",
    "    print(f\"URL에 접근하는 중 오류가 발생했습니다: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"\\n--- 오류 ---\")\n",
    "    print(f\"파일을 저장하는 중 오류가 발생했습니다: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- 알 수 없는 오류 ---\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74af44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FOMC 'Minutes' (의사록) 스크래핑 시작 (2020년 이후) ---\n",
      "대상 기간: 2020년 ~ 2025년\n",
      "데이터 저장 위치: C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2020-present.jsonl\n",
      "\n",
      "--- 2025년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025년 회의: 100%|██████████| 9/9 [00:10<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2024년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024년 회의: 100%|██████████| 8/8 [00:12<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2023년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023년 회의: 100%|██████████| 8/8 [00:13<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2022년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022년 회의: 100%|██████████| 8/8 [00:12<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2021년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021년 회의: 100%|██████████| 8/8 [00:11<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2020년 'Minutes' 데이터 처리 중 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020년 회의: 100%|██████████| 14/14 [00:16<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 스크래핑 완료 (2020년 이후) ---\n",
      "총 55개의 회의 항목 중 46개의 'Minutes' 문서를 수집했습니다.\n",
      "\n",
      "--- 모든 'Minutes' 작업 완료 ---\n",
      "최종 데이터가 'C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2020-present.jsonl' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FOMC 'Minutes' (의사록) 전용 크롤러 (2020 ~ 현재)\n",
    "#\n",
    "# [2025-10-30 v2 수정]\n",
    "# - 날짜 파싱(parse_meeting_date) 오류를 전면 수정했습니다.\n",
    "# - 메인 루프에서 '월'과 '일'을 정확히 찾아 함수로 전달합니다.\n",
    "#\n",
    "# 참고: 이 캘린더 페이지는 2020년 이후의 데이터만 포함하고 있습니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install requests beautifulsoup4 PyMuPDF tqdm`을 실행하여\n",
    "#    라이브러리를 설치하세요.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "CALENDAR_URL = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"fomc_minutes_2020-present.jsonl\") \n",
    "START_YEAR = 2020\n",
    "END_YEAR = datetime.datetime.now().year\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12,\n",
    "    'Jan/Feb': 1, 'Apr/May': 4, 'Oct/Nov': 10\n",
    "}\n",
    "\n",
    "# --- 2. 헬퍼 함수 (Helper Functions) ---\n",
    "\n",
    "def save_to_jsonl(data, outfile):\n",
    "    \"\"\"데이터를 JSON Lines (.jsonl) 파일에 추가합니다.\"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(outfile)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        with open(outfile, 'a', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"  [오류] 파일 쓰기 실패: {e}\")\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"지정된 URL의 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] URL에 접근할 수 없습니다: {url} ({e})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"불필요한 공백과 줄바꿈 문자를 정제합니다.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_link(link_url):\n",
    "    \"\"\"HTML 또는 PDF 링크에서 텍스트를 깨끗하게 추출합니다.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        full_url = urljoin(BASE_URL, link_url)\n",
    "        response = requests.get(full_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \".pdf\" in full_url.lower():\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Fed 웹사이트의 본문 내용 영역\n",
    "            content_div = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8')\n",
    "            if content_div:\n",
    "                text = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] 링크에서 텍스트 추출 실패: {full_url} ({e})\")\n",
    "        \n",
    "    time.sleep(0.5) \n",
    "    return clean_text(text)\n",
    "\n",
    "# [수정됨] 날짜 파싱 함수 (매개변수 및 로직 수정)\n",
    "def parse_meeting_date(month_text, day_text, year):\n",
    "    \"\"\" \"January\", \"28-29\", 2025를 datetime 객체로 변환 (회의 종료일 기준) \"\"\"\n",
    "    try:\n",
    "        month_str = month_text.strip()\n",
    "        \n",
    "        # \"22 (notation vote)\" -> \"22\"\n",
    "        # \"16-17*\" -> \"16-17\"\n",
    "        day_search_text = day_text.strip().split(' ')[0].replace('*', '')\n",
    "        \n",
    "        month = MONTH_MAP.get(month_str)\n",
    "        if not month: # \"Apr/May\" 같은 복합 월 처리\n",
    "            month_str_first = month_str.split('/')[0]\n",
    "            month = MONTH_MAP.get(month_str_first)\n",
    "            \n",
    "        # \"28-29\" -> 29, \"30-1\" -> 1, \"3\" -> 3\n",
    "        day = int(re.split(r'[-/]', day_search_text)[-1]) \n",
    "        \n",
    "        # 월이 넘어가는 \"30-1\" 같은 케이스 처리 (Apr/May)\n",
    "        if month_str == 'Apr/May' and day == 1:\n",
    "            month = 5 # 5월 1일\n",
    "        elif month_str == 'Jan/Feb' and day == 1:\n",
    "            month = 2 # 2월 1일\n",
    "        elif month_str == 'Oct/Nov' and day == 1:\n",
    "            month = 11 # 11월 1일\n",
    "\n",
    "        return datetime.datetime(year, month, day)\n",
    "    except Exception as e:\n",
    "         # print(f\"  [경고] 날짜 파싱 실패: '{month_str}', '{day_text}' ({year}년) - {e}\")\n",
    "         return None\n",
    "\n",
    "# --- 3. 메인 스크래핑 함수 (수정됨) ---\n",
    "def scrape_all_minutes(output_file):\n",
    "    print(f\"--- FOMC 'Minutes' (의사록) 스크래핑 시작 (2020년 이후) ---\")\n",
    "    print(f\"대상 기간: {START_YEAR}년 ~ {END_YEAR}년\")\n",
    "    print(f\"데이터 저장 위치: {output_file}\\n\")\n",
    "    \n",
    "    soup = get_soup(CALENDAR_URL)\n",
    "    if not soup:\n",
    "        print(\"[치명적 오류] 캘린더 페이지에 접근할 수 없습니다. 스크립트를 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    year_panels = soup.find_all('div', class_='panel-default')\n",
    "    scraped_count = 0\n",
    "    total_meetings = 0\n",
    "    \n",
    "    if not year_panels:\n",
    "        print(\"[치명적 오류] 'panel-default' 클래스를 찾지 못했습니다. HTML 구조가 다시 변경된 것 같습니다.\")\n",
    "        return\n",
    "\n",
    "    for year_panel in year_panels:\n",
    "        h4 = year_panel.find('h4')\n",
    "        if not h4 or not h4.text:\n",
    "            continue\n",
    "        \n",
    "        year_match = re.search(r'(\\d{4})', h4.text)\n",
    "        if not year_match:\n",
    "            continue\n",
    "            \n",
    "        year = int(year_match.group(1))\n",
    "            \n",
    "        if not (START_YEAR <= year <= END_YEAR):\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- {year}년 'Minutes' 데이터 처리 중 ---\")\n",
    "        \n",
    "        meetings = year_panel.find_all('div', class_='fomc-meeting')\n",
    "        \n",
    "        for meeting in tqdm(meetings, desc=f\"{year}년 회의\"):\n",
    "            total_meetings += 1 # 총 회의 수 카운트\n",
    "            \n",
    "            # [수정됨] '월'과 '일'을 별도로 찾음\n",
    "            month_element = meeting.find('div', class_='fomc-meeting__month')\n",
    "            date_element = meeting.find('div', class_='fomc-meeting__date')\n",
    "            \n",
    "            # '월' 또는 '일' 정보가 없으면 이 회의는 건너뜀\n",
    "            if not month_element or not date_element:\n",
    "                # print(f\"  [정보] {year}년 - '월' 또는 '일' 정보가 없는 항목(예: 각주) 건너뜀\")\n",
    "                continue\n",
    "                \n",
    "            month_text = month_element.get_text(strip=True)\n",
    "            date_text = date_element.get_text(strip=True)\n",
    "\n",
    "            # [수정됨] 날짜 파싱\n",
    "            meeting_date = parse_meeting_date(month_text, date_text, year)\n",
    "            \n",
    "            # 날짜 파싱에 실패하면 건너뜀\n",
    "            if meeting_date is None:\n",
    "                print(f\"  [경고] {year}년 '{month_text} {date_text}' - 날짜 파싱 실패. 건너뜁니다.\")\n",
    "                continue \n",
    "\n",
    "            minutes_div = meeting.find('div', class_='fomc-meeting__minutes')\n",
    "            minutes_link = None\n",
    "            if minutes_div:\n",
    "                minutes_link = minutes_div.find('a') \n",
    "\n",
    "            if minutes_link:\n",
    "                link_url = minutes_link.get('href')\n",
    "                \n",
    "                if not link_url.startswith('http') and not link_url.startswith('/'):\n",
    "                    link_url = f\"/monetarypolicy/fomc/minutes/{link_url}\"\n",
    "\n",
    "                source_type = \"FOMC Minutes\"\n",
    "                text = extract_text_from_link(link_url)\n",
    "                \n",
    "                if text:\n",
    "                    data = {\n",
    "                        \"date\": meeting_date.strftime('%Y-%m-%d'), \n",
    "                        \"year\": year,\n",
    "                        \"source_type\": source_type,\n",
    "                        \"url\": urljoin(BASE_URL, link_url),\n",
    "                        \"text\": text\n",
    "                    }\n",
    "                    save_to_jsonl(data, output_file)\n",
    "                    scraped_count += 1\n",
    "                else:\n",
    "                    print(f\"  [경고] {year} ({date_text}) - 텍스트 추출 실패 (링크: {link_url})\")\n",
    "            # else:\n",
    "                # (주석 처리) 'Minutes' 링크가 없는 회의(예: 10월 28-29일)는 조용히 건너뜀\n",
    "                # print(f\"  > {year} ({month_text} {date_text}) - 'Minutes' 링크 없음 (예정된 회의 등)\")\n",
    "                        \n",
    "    print(f\"\\n--- 스크래핑 완료 (2020년 이후) ---\")\n",
    "    print(f\"총 {total_meetings}개의 회의 항목 중 {scraped_count}개의 'Minutes' 문서를 수집했습니다.\")\n",
    "\n",
    "# --- 4. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "        print(f\"'{DOWNLOADS_DIR}' 폴더를 생성했습니다.\")\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"기존 파일 '{OUTPUT_FILE}'을 삭제했습니다. 새로 수집합니다.\")\n",
    "    \n",
    "    scrape_all_minutes(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n--- 모든 'Minutes' 작업 완료 ---\")\n",
    "    print(f\"최종 데이터가 '{OUTPUT_FILE}' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489e21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FOMC 'Minutes' (의사록) 과거 데이터 스크래핑 시작 ---\n",
      "대상 기간: 2000년 ~ 2019년\n",
      "데이터 저장 위치: C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2000-2019.jsonl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집: 100%|██████████| 20/20 [00:54<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 스크래핑 완료 (2000-2019) ---\n",
      "총 137개의 회의 항목 중 43개의 'Minutes' 문서를 수집했습니다.\n",
      "\n",
      "--- 모든 'Minutes' 작업 완료 ---\n",
      "최종 데이터가 'C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2000-2019.jsonl' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FOMC 'Minutes' (의사록) 전용 크롤러 (2000 ~ 2019년 과거 데이터)\n",
    "#\n",
    "# [2025-10-30 v3 수정]\n",
    "# - 날짜 파싱 로직을 (MONTH_MAP) 기준으로 강화\n",
    "# - \"pages...\", \"Part 1\" 등 불필요한 행을 건너뛰도록 수정\n",
    "# - [핵심] re.match() (처음부터) -> re.search() (어디서든)로 변경\n",
    "#\n",
    "# 참고: 이 스크립트는 2000년부터 2019년까지의 데이터를 수집합니다.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install requests beautifulsoup4 PyMuPDF tqdm`을 실행하여\n",
    "#    라이브러리를 설치하세요.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "HISTORICAL_URL_TEMPLATE = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"fomc_minutes_2000-2019.jsonl\")\n",
    "START_YEAR = 2000 \n",
    "END_YEAR = 2019\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12,\n",
    "    'Jan.': 1, 'Feb.': 2, 'Mar.': 3, 'Apr.': 4, 'Jun.': 6, 'Jul.': 7, 'Aug.': 8, 'Sept.': 9, 'Oct.': 10, 'Nov.': 11, 'Dec.': 12\n",
    "}\n",
    "\n",
    "# --- 2. 헬퍼 함수 (Helper Functions) ---\n",
    "\n",
    "def save_to_jsonl(data, outfile):\n",
    "    \"\"\"데이터를 JSON Lines (.jsonl) 파일에 추가합니다.\"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(outfile)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        with open(outfile, 'a', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"  [오류] 파일 쓰기 실패: {e}\")\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"지정된 URL의 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding \n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] URL에 접근할 수 없습니다: {url} ({e})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"불필요한 공백과 줄바꿈 문자를 정제합니다.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_link(link_url):\n",
    "    \"\"\"HTML 또는 PDF 링크에서 텍스트를 깨끗하게 추출합니다.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        full_url = urljoin(BASE_URL, link_url)\n",
    "        response = requests.get(full_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \".pdf\" in full_url.lower():\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content = soup.find('div', id='article')\n",
    "            if not content:\n",
    "                content = soup.find('body')\n",
    "            \n",
    "            if content:\n",
    "                # 스크립트와 스타일 태그 제거\n",
    "                for script_or_style in content(['script', 'style']):\n",
    "                    script_or_style.decompose()\n",
    "                text = content.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] 링크에서 텍스트 추출 실패: {full_url} ({e})\")\n",
    "        \n",
    "    time.sleep(0.5) \n",
    "    return clean_text(text)\n",
    "\n",
    "def parse_historical_date(month_str, day_str, year):\n",
    "    \"\"\" \"January\", \"30-31\", 2018을 datetime 객체로 변환 (회의 종료일 기준) \"\"\"\n",
    "    try:\n",
    "        # \"Jan.\" -> \"Jan\"\n",
    "        month_key = month_str.strip('.')\n",
    "        month = MONTH_MAP.get(month_key)\n",
    "        \n",
    "        if not month:\n",
    "             # \"Jan\" 같은 축약형도 처리\n",
    "            for k, v in MONTH_MAP.items():\n",
    "                if k.startswith(month_key):\n",
    "                    month = v\n",
    "                    break\n",
    "        if not month:\n",
    "             raise ValueError(f\"알 수 없는 월: {month_str}\")\n",
    "\n",
    "        # \"30-31\" -> 31, \"1\" -> 1\n",
    "        day = int(re.split(r'[-/]', day_str)[-1]) \n",
    "        \n",
    "        return datetime.datetime(year, month, day)\n",
    "    except Exception as e:\n",
    "         # print(f\"  [경고] 날짜 파싱 실패: '{month_str} {day_str}' ({year}년) - {e}\")\n",
    "         return None\n",
    "\n",
    "# --- 3. 메인 스크래핑 함수 (Historical) ---\n",
    "def scrape_historical_minutes(output_file):\n",
    "    print(f\"--- FOMC 'Minutes' (의사록) 과거 데이터 스크래핑 시작 ---\")\n",
    "    print(f\"대상 기간: {START_YEAR}년 ~ {END_YEAR}년\")\n",
    "    print(f\"데이터 저장 위치: {output_file}\\n\")\n",
    "    \n",
    "    scraped_count = 0\n",
    "    total_meetings = 0\n",
    "    \n",
    "    # --- [수정] 월(Month) 이름으로만 시작하는 행을 찾기 위한 정규식 생성 ---\n",
    "    escaped_months = [re.escape(k) for k in MONTH_MAP.keys()]\n",
    "    month_pattern = '|'.join(escaped_months)\n",
    "    \n",
    "    # [v3 수정] ^(시작) 앵커를 제거하고 \\s* (공백)도 제거\n",
    "    # (group 1): 월, \\s+: 공백, (group 2): 날짜\n",
    "    date_regex = re.compile(r'(' + month_pattern + r')\\s+([\\d-]+)')\n",
    "    # --- [수정 완료] ---\n",
    "    \n",
    "    # START_YEAR부터 END_YEAR까지 역순으로 순회\n",
    "    for year in tqdm(range(END_YEAR, START_YEAR - 1, -1), desc=\"연도별 수집\"):\n",
    "        \n",
    "        historical_url = HISTORICAL_URL_TEMPLATE.format(year=year)\n",
    "        \n",
    "        soup = get_soup(historical_url)\n",
    "        if not soup:\n",
    "            print(f\"  [경고] {year}년 페이지를 건너뜁니다 (접근 불가)\")\n",
    "            continue\n",
    "            \n",
    "        article = soup.find('div', id='article')\n",
    "        if not article:\n",
    "            print(f\"  [경고] {year}년 페이지를 건너뜁니다 (article div 없음)\")\n",
    "            continue\n",
    "\n",
    "        rows = article.find_all(['tr', 'p'])\n",
    "        \n",
    "        for row in rows:\n",
    "            row_text = row.get_text(strip=True)\n",
    "\n",
    "            # [v3 수정] .match() -> .search()\n",
    "            # 문자열 '어디에서든' \"월 + 날짜\" 패턴을 찾음\n",
    "            date_match = date_regex.search(row_text)\n",
    "            \n",
    "            if not date_match:\n",
    "                # \"pages 25\", \"Part 1\" 등은 `month_pattern`에 없으므로 무시됨\n",
    "                continue \n",
    "            \n",
    "            month_str = date_match.group(1) # \"January\"\n",
    "            day_str = date_match.group(2) # \"30-31\"\n",
    "            \n",
    "            meeting_date = parse_historical_date(month_str, day_str, year)\n",
    "            if meeting_date is None:\n",
    "                continue\n",
    "                \n",
    "            total_meetings += 1\n",
    "            \n",
    "            # 2. 'Minutes' 링크 찾기\n",
    "            minutes_link = None\n",
    "            links = row.find_all('a')\n",
    "            for link in links:\n",
    "                link_text = link.get_text(strip=True).lower()\n",
    "                if 'minutes' in link_text:\n",
    "                    minutes_link = link\n",
    "                    break\n",
    "            \n",
    "            if minutes_link:\n",
    "                link_url = minutes_link.get('href')\n",
    "                \n",
    "                if not link_url.startswith('http') and not link_url.startswith('/'):\n",
    "                    if '.htm' in link_url and not link_url.startswith('/'):\n",
    "                         link_url = f\"/monetarypolicy/fomc/minutes/{link_url}\"\n",
    "                \n",
    "                source_type = \"FOMC Minutes (Historical)\"\n",
    "                text = extract_text_from_link(link_url)\n",
    "                \n",
    "                if text:\n",
    "                    data = {\n",
    "                        \"date\": meeting_date.strftime('%Y-%m-%d'), \n",
    "                        \"year\": year,\n",
    "                        \"source_type\": source_type,\n",
    "                        \"url\": urljoin(BASE_URL, link_url),\n",
    "                        \"text\": text\n",
    "                    }\n",
    "                    save_to_jsonl(data, output_file)\n",
    "                    scraped_count += 1\n",
    "                else:\n",
    "                    print(f\"  [경고] {year} ({month_str} {day_str}) - 텍스트 추출 실패 (링크: {link_url})\")\n",
    "                        \n",
    "    print(f\"\\n--- 스크래핑 완료 (2000-2019) ---\")\n",
    "    print(f\"총 {total_meetings}개의 회의 항목 중 {scraped_count}개의 'Minutes' 문서를 수집했습니다.\")\n",
    "\n",
    "# --- 4. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "        print(f\"'{DOWNLOADS_DIR}' 폴더를 생성했습니다.\")\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"기존 파일 '{OUTPUT_FILE}'을 삭제했습니다. 새로 수집합니다.\")\n",
    "    \n",
    "    scrape_historical_minutes(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n--- 모든 'Minutes' 작업 완료 ---\")\n",
    "    print(f\"최종 데이터가 '{OUTPUT_FILE}' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d213d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2019년 '목록' 페이지 HTML 디버깅 시작 ---\n",
      "대상 URL: https://www.federalreserve.gov/monetarypolicy/fomchistorical2019.htm\n",
      "\n",
      "--- 성공! ---\n",
      "HTML 원본 파일이 'C:\\Users\\82109\\Downloads\\statements\\debug_historical_2019.html' 경로에 저장되었습니다.\n",
      "이 파일을 저에게 업로드해주세요.\n"
     ]
    }
   ],
   "source": [
    "# [디버깅 스크립트]\n",
    "# 2000-2019년 과거 데이터 목록 페이지(2019년)의 HTML을 저장합니다.\n",
    "# 0개가 수집되는 원인을 파악하기 위함입니다.\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# --- 1. 설정 ---\n",
    "# 2019년 페이지를 샘플로 저장\n",
    "YEAR_TO_DEBUG = 2019\n",
    "HISTORICAL_URL_TEMPLATE = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "DEBUG_URL = HISTORICAL_URL_TEMPLATE.format(year=YEAR_TO_DEBUG)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# --- 2. 저장 위치 ---\n",
    "# (이 스크립트를 실행하는 폴더와 동일한 위치)\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, f\"debug_historical_{YEAR_TO_DEBUG}.html\")\n",
    "\n",
    "# --- 3. 메인 실행 ---\n",
    "def main():\n",
    "    print(f\"--- {YEAR_TO_DEBUG}년 '목록' 페이지 HTML 디버깅 시작 ---\")\n",
    "    print(f\"대상 URL: {DEBUG_URL}\")\n",
    "\n",
    "    try:\n",
    "        # 1. 폴더 생성\n",
    "        if not os.path.exists(DOWNLOADS_DIR):\n",
    "            os.makedirs(DOWNLOADS_DIR)\n",
    "\n",
    "        # 2. 웹페이지 요청\n",
    "        response = requests.get(DEBUG_URL, headers=HEADERS)\n",
    "        response.raise_for_status() # 오류가 있으면 예외 발생\n",
    "        \n",
    "        # 3. HTML 텍스트 저장\n",
    "        # (BeautifulSoup 파싱 안함! 원본 그대로 저장)\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "            \n",
    "        print(f\"\\n--- 성공! ---\")\n",
    "        print(f\"HTML 원본 파일이 '{OUTPUT_FILE}' 경로에 저장되었습니다.\")\n",
    "        print(\"이 파일을 저에게 업로드해주세요.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n[오류] 페이지에 접근할 수 없습니다: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"\\n[오류] 파일 저장에 실패했습니다: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[알 수 없는 오류] {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c04777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 파일 'C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2000-2019.jsonl'을 삭제했습니다. 새로 수집합니다.\n",
      "--- FOMC 'Minutes' (의사록) 과거 데이터 스크래핑 시작 (v6) ---\n",
      "대상 기간: 2000년 ~ 2019년\n",
      "데이터 저장 위치: C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2000-2019.jsonl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집:  35%|███▌      | 7/20 [01:07<02:09,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [경고] 날짜 파싱 실패: 'July 31-August 1  Meeting - 2012' (2012년) - invalid literal for int() with base 10: ''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "연도별 수집: 100%|██████████| 20/20 [03:11<00:00,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 스크래핑 완료 (2000-2019) ---\n",
      "총 185개의 회의 항목 중 168개의 'Minutes' 문서를 수집했습니다.\n",
      "\n",
      "--- 모든 'Minutes' 작업 완료 ---\n",
      "최종 데이터가 'C:\\Users\\82109\\Downloads\\statements\\fomc_minutes_2000-2019.jsonl' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FOMC 'Minutes' (의사록) 전용 크롤러 (2000 ~ 2019년 과거 데이터)\n",
    "#\n",
    "# [2025-10-30 v6 수정]\n",
    "# - 2007-2019년의 'div.panel' 구조 (debug_historical_2019.html 기반)\n",
    "# - 2000-2006년의 'p/tr' 구조\n",
    "#\n",
    "# [v6 핵심 수정]\n",
    "# - find_minutes_link 함수가 '링크 텍스트'(예: \"HTML\") 대신\n",
    "#   'URL 주소(href)'에 'fomcminutes'가 포함되어 있는지 검사하도록 수정.\n",
    "# - 2000-2006년 방식을 위해 'record of policy actions' 텍스트 검사 로직은 유지.\n",
    "#\n",
    "# [중요] 실행 전 설정:\n",
    "# 1. 터미널에서 `pip install requests beautifulsoup4 PyMuPDF tqdm`을 실행하여\n",
    "#    라이브러리를 설치하세요.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- 1. 설정 (Configuration) ---\n",
    "BASE_URL = \"https://www.federalreserve.gov\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "HISTORICAL_URL_TEMPLATE = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "DOWNLOADS_DIR = os.path.join(os.path.expanduser('~'), 'Downloads', 'statements')\n",
    "OUTPUT_FILE = os.path.join(DOWNLOADS_DIR, \"fomc_minutes_2000-2019.jsonl\")\n",
    "START_YEAR = 2000 \n",
    "END_YEAR = 2019\n",
    "\n",
    "MONTH_MAP = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "    'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12,\n",
    "    'Jan.': 1, 'Feb.': 2, 'Mar.': 3, 'Apr.': 4, 'Jun.': 6, 'Jul.': 7, 'Aug.': 8, 'Sept.': 9, 'Oct.': 10, 'Nov.': 11, 'Dec.': 12,\n",
    "    'April/May': 4\n",
    "}\n",
    "\n",
    "# --- 2. 헬퍼 함수 (Helper Functions) ---\n",
    "\n",
    "def save_to_jsonl(data, outfile):\n",
    "    \"\"\"데이터를 JSON Lines (.jsonl) 파일에 추가합니다.\"\"\"\n",
    "    try:\n",
    "        output_dir = os.path.dirname(outfile)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        with open(outfile, 'a', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"  [오류] 파일 쓰기 실패: {e}\")\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"지정된 URL의 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        response.encoding = response.apparent_encoding \n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  [오류] URL에 접근할 수 없습니다: {url} ({e})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"불필요한 공백과 줄바꿈 문자를 정제합니다.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_link(link_url):\n",
    "    \"\"\"HTML 또는 PDF 링크에서 텍스트를 깨끗하게 추출합니다.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        full_url = urljoin(BASE_URL, link_url)\n",
    "        \n",
    "        response = requests.get(full_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if \".pdf\" in full_url.lower():\n",
    "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text() + \"\\n\"\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # [사용자님 확인] <div id=\"article\">이 본문이 맞습니다.\n",
    "            content = soup.find('div', id='article') \n",
    "            if not content:\n",
    "                content = soup.find('body')\n",
    "            \n",
    "            if content:\n",
    "                for script_or_style in content(['script', 'style']):\n",
    "                    script_or_style.decompose()\n",
    "                text = content.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] 링크에서 텍스트 추출 실패: {full_url} ({e})\")\n",
    "        \n",
    "    time.sleep(0.5) \n",
    "    return clean_text(text)\n",
    "\n",
    "def parse_date_from_text(text, year):\n",
    "    \"\"\" 텍스트에서 '월'과 '일'을 찾아 datetime 객체로 변환 \"\"\"\n",
    "    try:\n",
    "        escaped_months = [re.escape(k) for k in MONTH_MAP.keys()]\n",
    "        month_pattern = '|'.join(escaped_months)\n",
    "        date_regex = re.compile(r'(' + month_pattern + r')\\s+([\\d-]+)')\n",
    "        \n",
    "        date_match = date_regex.search(text)\n",
    "        if not date_match:\n",
    "            return None \n",
    "\n",
    "        month_str_key = date_match.group(1) # \"April/May\" 또는 \"Jan.\"\n",
    "        day_str = date_match.group(2) # \"30-31\"\n",
    "        \n",
    "        month = MONTH_MAP.get(month_str_key)\n",
    "        \n",
    "        if month_str_key == 'April/May':\n",
    "            month = 4\n",
    "            day = int(re.split(r'[-/]', day_str)[-1])\n",
    "            if day == 1: \n",
    "                month = 5\n",
    "        else:\n",
    "            if not month:\n",
    "                 # \"Jan.\" -> \"Jan\"\n",
    "                 month_str_key = month_str_key.strip('.')\n",
    "                 month = MONTH_MAP.get(month_str_key)\n",
    "                 \n",
    "            if not month:\n",
    "                 raise ValueError(f\"알 수 없는 월: {date_match.group(1)}\")\n",
    "            day = int(re.split(r'[-/]', day_str)[-1]) \n",
    "        \n",
    "        return datetime.datetime(year, month, day)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"  [경고] 날짜 파싱 실패: '{text}' ({year}년) - {e}\")\n",
    "         return None\n",
    "\n",
    "# --- [v6 수정] ---\n",
    "def find_minutes_link(element):\n",
    "    \"\"\" 주어진 HTML 요소(element) 안에서 'Minutes' 링크를 찾음 \"\"\"\n",
    "    links = element.find_all('a')\n",
    "    for link in links:\n",
    "        href = link.get('href', '')\n",
    "        link_text = link.get_text(strip=True).lower()\n",
    "\n",
    "        # [v6 수정] Priority 1: URL에 'fomcminutes'가 있는지 (2007-2019년 방식)\n",
    "        # 예: /monetarypolicy/fomcminutes20190130.htm\n",
    "        if 'fomcminutes' in href and 'transcript' not in href:\n",
    "            return href\n",
    "        \n",
    "        # [v6 수정] Priority 2: 링크 텍스트에 \"minutes\" 또는 \"record\"가 있는지 (2000-2006년 방식)\n",
    "        is_minutes = 'minutes' in link_text\n",
    "        is_record = 'record of policy actions' in link_text\n",
    "        is_not_transcript = 'transcript' not in link_text\n",
    "        \n",
    "        if (is_minutes or is_record) and is_not_transcript:\n",
    "             return href\n",
    "             \n",
    "    return None # 링크 못 찾음\n",
    "\n",
    "# --- 3. 메인 스크래핑 함수 (v5 로직 유지, v6 함수 적용) ---\n",
    "def scrape_historical_minutes(output_file):\n",
    "    print(f\"--- FOMC 'Minutes' (의사록) 과거 데이터 스크래핑 시작 (v6) ---\")\n",
    "    print(f\"대상 기간: {START_YEAR}년 ~ {END_YEAR}년\")\n",
    "    print(f\"데이터 저장 위치: {output_file}\\n\")\n",
    "    \n",
    "    scraped_count = 0\n",
    "    total_meetings = 0\n",
    "    \n",
    "    for year in tqdm(range(END_YEAR, START_YEAR - 1, -1), desc=\"연도별 수집\"):\n",
    "        \n",
    "        historical_url = HISTORICAL_URL_TEMPLATE.format(year=year)\n",
    "        soup = get_soup(historical_url)\n",
    "        if not soup:\n",
    "            print(f\"  [경고] {year}년 페이지를 건너뜁니다 (접근 불가)\")\n",
    "            continue\n",
    "            \n",
    "        article = soup.find('div', id='article')\n",
    "        if not article:\n",
    "            print(f\"  [경고] {year}년 페이지를 건너뜁니다 (article div 없음)\")\n",
    "            continue\n",
    "\n",
    "        # --- 모드 1: 2007-2019년 방식 ('div.panel' 구조) ---\n",
    "        meetings_panels = article.find_all('div', class_='panel-default')\n",
    "        \n",
    "        if meetings_panels:\n",
    "            \n",
    "            for panel in meetings_panels:\n",
    "                # 1. 날짜 찾기 (h5 태그)\n",
    "                h5 = panel.find('h5')\n",
    "                if not h5: continue\n",
    "                \n",
    "                date_text = h5.get_text(strip=True) \n",
    "                meeting_date = parse_date_from_text(date_text, year)\n",
    "                if meeting_date is None:\n",
    "                    continue\n",
    "                \n",
    "                total_meetings += 1\n",
    "                \n",
    "                # 2. 'Minutes' 링크 찾기 (v6 함수 사용)\n",
    "                link_url = find_minutes_link(panel)\n",
    "                \n",
    "                if link_url:\n",
    "                    text = extract_text_from_link(link_url)\n",
    "                    if text:\n",
    "                        data = {\n",
    "                            \"date\": meeting_date.strftime('%Y-%m-%d'), \n",
    "                            \"year\": year,\n",
    "                            \"source_type\": \"FOMC Minutes (Historical)\",\n",
    "                            \"url\": urljoin(BASE_URL, link_url),\n",
    "                            \"text\": text\n",
    "                        }\n",
    "                        save_to_jsonl(data, output_file)\n",
    "                        scraped_count += 1\n",
    "        \n",
    "        # --- 모드 2: 2000-2006년 방식 ('p' 또는 'tr' 구조) ---\n",
    "        else:\n",
    "            rows = article.find_all(['tr', 'p'])\n",
    "            \n",
    "            for row in rows:\n",
    "                row_text = row.get_text(strip=True)\n",
    "                \n",
    "                # 1. 날짜 찾기\n",
    "                meeting_date = parse_date_from_text(row_text, year)\n",
    "                if meeting_date is None:\n",
    "                    continue \n",
    "                \n",
    "                total_meetings += 1\n",
    "                \n",
    "                # 2. 'Minutes' 링크 찾기 (v6 함수 사용)\n",
    "                link_url = find_minutes_link(row)\n",
    "                \n",
    "                if link_url:\n",
    "                    text = extract_text_from_link(link_url)\n",
    "                    if text:\n",
    "                        data = {\n",
    "                            \"date\": meeting_date.strftime('%Y-%m-%d'), \n",
    "                            \"year\": year,\n",
    "                            \"source_type\": \"FOMC Minutes (Historical)\",\n",
    "                            \"url\": urljoin(BASE_URL, link_url),\n",
    "                            \"text\": text\n",
    "                        }\n",
    "                        save_to_jsonl(data, output_file)\n",
    "                        scraped_count += 1\n",
    "                        \n",
    "    print(f\"\\n--- 스크래핑 완료 (2000-2019) ---\")\n",
    "    print(f\"총 {total_meetings}개의 회의 항목 중 {scraped_count}개의 'Minutes' 문서를 수집했습니다.\")\n",
    "\n",
    "# --- 4. 메인 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOWNLOADS_DIR):\n",
    "        os.makedirs(DOWNLOADS_DIR)\n",
    "        print(f\"'{DOWNLOADS_DIR}' 폴더를 생성했습니다.\")\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "        print(f\"기존 파일 '{OUTPUT_FILE}'을 삭제했습니다. 새로 수집합니다.\")\n",
    "    \n",
    "    scrape_historical_minutes(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n--- 모든 'Minutes' 작업 완료 ---\")\n",
    "    print(f\"최종 데이터가 '{OUTPUT_FILE}' 파일에 저장되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01211da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
